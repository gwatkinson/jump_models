defaults:
  - /model/optimizer/adamw.yaml@tox21.model.optimizer
  - /model/scheduler/cosine_annealing_with_warmup.yaml@tox21.model.scheduler
  - /trainer/gpu.yaml@tox21.trainer
  - /callbacks/eval/ogb.yaml@tox21.callbacks

tox21:
  model:
    _target_: src.eval.ogb.module.Tox21Module

    lr: 5e-4

    scheduler:
      warmup_epochs: 3
      max_epochs: ${eval.tox21.trainer.max_epochs}

    example_input_path: ${model.example_input_path}
    # example_input_path: ${paths.data_root_dir}/jump/models/eval/test/example.pt

    split_lr_in_groups: False

  datamodule:
    _target_: src.eval.ogb.datamodule.Tox21DataModule

    root_dir: ${paths.data_root_dir}/ogb/

    batch_size: 256
    num_workers: 16
    pin_memory: False
    prefetch_factor: 2

    compound_transform: ${data.compound_transform}

    split_type: scaffold

    smiles_col: smiles
    targets: null
    # collate_fn: null

    use_cache: False


  trainer:
    default_root_dir: ${paths.output_dir}/eval/ogb/tox21/
    devices: ${trainer.devices}
    min_epochs: 0
    max_epochs: 100
    log_every_n_steps: 1
    # gradient_clip_val: 0.5
    num_sanity_val_steps: 1
    check_val_every_n_epoch: 1

  evaluator:
    _target_: src.eval.evaluators.Evaluator
    name: Tox21Module
    visualize_kwargs: null


  callbacks:
    early_stopping:
      monitor: ogb/tox21/val/loss

    model_checkpoint:
      monitor: ogb/tox21/val/loss
      dirpath: ${eval.tox21.trainer.default_root_dir}/checkpoints

    wandb_plotter:
      prefix: ogb/tox21

