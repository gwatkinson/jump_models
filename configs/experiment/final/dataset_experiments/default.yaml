# @package _global_

defaults:
  - override /data: jump_cl.yaml
  - override /model: jump_cl.yaml
  - override /model/image_encoder: resnet34.yaml
  - override /model/molecule_encoder: pna_pretrained.yaml
  - override /model/criterion: ntxent.yaml
  - override /model/optimizer: adamw.yaml
  - override /model/scheduler: cosine_annealing_with_warmup.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml
  - override /logger: wandb.yaml
  - override /eval: fast.yaml


tags:
  - final_experiments
  - pretrained
  - ntxent
  - single_view
  - resnet34
  - pna

seed: 12345

compile: False

evaluate: False

model:
  embedding_dim: 768
  lr: 3e-4

  model:
    criterion:
      temperature: 0.5

  scheduler:
    warmup_epochs: 10
    max_epochs: ${trainer.max_epochs}

  example_input_path: null


data:
  batch_size: 196
  num_workers: 16
  prefetch_factor: 2
  drop_last: True

  transform:
    size: 512

  splitter:
    _target_: src.splitters.RandomSplitter
    train: 32_768
    test: 8192
    val: 4096
    retrieval: 4096

  split_path: ${paths.split_path}/random_split/

  train_ids_name: ???


trainer:
  min_epochs: 0
  max_epochs: 200
  log_every_n_steps: 1
  num_sanity_val_steps: 1

  devices: ???


callbacks:
  wandb_watcher:
    log_freq: 100
    watch_log: all

  early_stopping:
    monitor: "val/loss"
    patience: 25
    min_delta: 0
    mode: "min"


logger:
  wandb:
    tags: ${tags}
    name: ???
    project: final_experiments
    group: dataset_experiment
    job_type: pretrain
    log_model: True
