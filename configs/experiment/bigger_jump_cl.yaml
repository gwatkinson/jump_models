# @package _global_

# to execute this experiment run:
# python train.py experiment=jump_cl_bigger

defaults:
  - override /data: bigger_jump_cl.yaml
  - override /model: bigger_jump_cl.yaml
  - override /callbacks: not_rich_default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["bigger_jump_cl", "jump", "simple_contrastive_training", "pretrained_gin", "pretrained_resnet50"]

seed: 12345

compile: False

trainer:
  min_epochs: 35
  max_epochs: 100
  log_every_n_steps: 50
  gradient_clip_val: 0.5
  num_sanity_val_steps: 2

logger:
  wandb:
    tags: ${tags}
    project: "jump_models"
    group: "jump_models"
    job_type: ""
    log_model: True
