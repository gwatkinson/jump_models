# @package _global_

# to execute this experiment run:
# python train.py experiment=jump_cl

defaults:
  - override /data: jump_cl.yaml
  - override /model: jump_cl.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["med_jump_cl", "simple_contrastive_training", "pretrained_gin_infomax", "pretrained_resnet18"]

seed: 12345

compile: False

trainer:
  min_epochs: 20
  max_epochs: 50
  log_every_n_steps: 1
  # gradient_clip_val: 0.5
  num_sanity_val_steps: 2


data:
  batch_size: 128
  num_workers: 24
  pin_memory: False

  transform:
    size: 128

  splitter:
    train: 25_000
    test: 3000
    val: 2000

  split_path: ${paths.split_path}/med_jump_cl/

  compound_transform: dgllife_gin.yaml


model:
  defaults:
    image_encoder: resnet18
    molecule_encoder: gin_supervised_masking
    optimizer: adam
    scheduler: exponential

  embedding_dim: 256

  optimizer:
    lr: 1e-3

  scheduler:
    gamma: 0.9

  example_input_path: /projects/cpjump1/jump/models/example_batch/simple_jump_cl/batch.pth


logger:
  wandb:
    tags: ${tags}
    project: "jump_models"
    group: "jump_models"
    job_type: ""
    log_model: True
