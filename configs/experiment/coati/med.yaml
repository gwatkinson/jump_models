# @package _global_

defaults:
  - override /data: jump_cl.yaml
  - override /model: jump_cl.yaml
  - override /model/image_encoder: resnet34.yaml
  - override /model/molecule_encoder: coati_tall.yaml
  - override /model/optimizer: adam.yaml
  - override /model/scheduler: on_plateau_with_warmup.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml
  - override /logger: many_loggers.yaml
  - override /eval: evaluators.yaml


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags:
  - med_jump_cl
  - nlp_coati
  - clip_like
  - ${model.molecule_encoder.pretrained_name}
  - ${model.image_encoder.instance_model_name}

seed: 12345

compile: False

evaluate: True

trainer:
  min_epochs: 5
  max_epochs: 100
  log_every_n_steps: 1
  # gradient_clip_val: 0.5
  num_sanity_val_steps: 1
  # reload_dataloaders_every_n_epochs: 1

  devices: ???

data:
  batch_size: 256
  num_workers: 24
  prefetch_factor: 3
  drop_last: True

  transform:
    size: 128

  splitter:
    train: 24_576
    test: 3_072
    val: 2_048

  split_path: ${paths.split_path}/fp_med4/


model:
  embedding_dim: 512
  lr: 1e-4

  scheduler:
    warmup_steps: [3]
    cooldown: 3
    factor: 0.6
    patience: 7
    min_lr: 1.0e-6
    threshold: 1.0e-4

  optimizer:
    weight_decay: 3e-4

  example_input_path: null
  # example_input_path: ${paths.data_root_dir}/jump/models/example_batch/simple_jump_cl/batch.pth

callbacks:
  wandb_watcher:
    log_freq: 100
    watch_log: all

  early_stopping:
    monitor: "val/loss"
    patience: 25
    min_delta: 0
    mode: "min"



logger:
  wandb:
    tags: ${tags}
    project: "coati_med"
    group: null
    job_type: ""
    log_model: True
