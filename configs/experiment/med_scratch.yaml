# @package _global_

# to execute this experiment run:
# python train.py experiment=med_jump_cl

defaults:
  - override /data: jump_cl.yaml
  - override /data/compound_transform: attentive_fp.yaml
  - override /model: jump_cl.yaml
  - override /model/image_encoder: vit_base.yaml
  - override /model/molecule_encoder: attentive_fp.yaml
  - override /model/optimizer: adam.yaml
  - override /model/scheduler: cosine_annealing_warm_restart.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml
  - override /logger: many_loggers.yaml
  - override /eval: evaluators.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags:
  - med_jump_cl
  - simple_contrastive_training
  - ${model.molecule_encoder.pretrained_name}
  - ${model.image_encoder.instance_model_name}

seed: 12345

compile: False

evaluate: True

trainer:
  min_epochs: 0
  max_epochs: 50
  log_every_n_steps: 1
  gradient_clip_val: null # 0.5
  num_sanity_val_steps: 1
  reload_dataloaders_every_n_epochs: 1

  devices: ???


data:
  batch_size: 512
  num_workers: 24
  prefetch_factor: 2

  transform:
    size: 384

  splitter:
    train: 25_000
    test: 3000
    val: 2000

  split_path: ${paths.split_path}/med_scratch/
  image_metadata_path: ${paths.metadata_path}/images_metadata_scratch.parquet
  compound_metadata_path: ${paths.metadata_path}/compound_dict_scratch.json


model:
  embedding_dim: 512
  lr: 1e-2

  monitor: val/loss  # metric to monitor for some schedulers
  interval: epoch    # epoch or step
  frequency: 1       # how often to call the scheduler wrt interval

  scheduler:
    T_0: 7
    T_mult: 2

  example_input_path: /projects/cpjump1/jump/models/example_batch/simple_jump_cl/batch.pth


callbacks:
  wandb_watcher:
    log_freq: 100
    watch_log: all


logger:
  wandb:
    tags: ${tags}
    project: "jump_models"
    group: "med_jump_cl"
    job_type: ""
    log_model: True
