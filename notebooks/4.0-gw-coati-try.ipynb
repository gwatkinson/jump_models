{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COATI NLP model for encoding molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from lightning import Callback, LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import Logger\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "\n",
    "from src import utils\n",
    "from src.coati.models.io import load_e3gnn_smiles_clip_e2e\n",
    "from src.modules.collate_fn import default_collate\n",
    "from src.modules.losses import InfoNCE\n",
    "from src.modules.molecules.coati import COATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpjump1 already mounted.\n",
      "cpjump2 already mounted.\n",
      "cpjump3 already mounted.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    if not Path(f\"../cpjump{i}/jump/\").exists():\n",
    "        print(f\"Mounting cpjump{i}...\")\n",
    "        os.system(f\"sshfs bioclust:/projects/cpjump{i}/ ../cpjump{i}\")\n",
    "    else:\n",
    "        print(f\"cpjump{i} already mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developping the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [\n",
    "    \"CC1CC2=CCOC2O1\",\n",
    "    \"OC1CC1(O)CC1CC1\",\n",
    "    \"CC1N2C=NCC12C#C\",\n",
    "    \"CC1COC11C(O)C1O\",\n",
    "    \"CC12OCC(CO1)C2=O\",\n",
    "    \"CC12CC(CO1)CC2=O\",\n",
    "    \"CCN=COC\",\n",
    "    \"CC1(CO)CO1\",\n",
    "    \"C(C#N)C(=O)N\",\n",
    "    \"CC(=O)OC=N\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from s3://terray-public/models/grande_closed.pkl\n",
      "Loading tokenizer may_closedparen from s3://terray-public/models/grande_closed.pkl\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 17.92M Total: 20.36M \n",
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n"
     ]
    }
   ],
   "source": [
    "model = COATI(\n",
    "    pretrained_name=\"grande_closed\",\n",
    "    out_dim=128,\n",
    "    padding_length=250,\n",
    "    freeze=False,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/notebooks/4.0-gw-coati-try.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/notebooks/4.0-gw-coati-try.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model(smiles)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/jump_models/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/modules/molecules/coati.py:102\u001b[0m, in \u001b[0;36mCOATI.forward\u001b[0;34m(self, smiles, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice:\n\u001b[1;32m    100\u001b[0m     tokens \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 102\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tokens)\n\u001b[1;32m    103\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_head(z)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/modules/molecules/coati.py:94\u001b[0m, in \u001b[0;36mCOATI.extract\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone\u001b[39m.\u001b[39;49mencode(idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer)\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/coati/models/encoding/smiles_xformer.py:102\u001b[0m, in \u001b[0;36mRotarySmilesTransformer.encode\u001b[0;34m(self, idx, tokenizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, idx, tokenizer):\n\u001b[1;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Only returns the vector of the [STOP] token which MUST be the last\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m    token before [PAD]\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxformer(idx)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m get_stop_token_embs(x, idx, tokenizer)\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/coati/models/encoding/smiles_xformer.py:319\u001b[0m, in \u001b[0;36mRotarySmilesTransformer.xformer\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    317\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(idx)\n\u001b[1;32m    318\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh:\n\u001b[0;32m--> 319\u001b[0m     x \u001b[39m=\u001b[39m block(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memb)\n\u001b[1;32m    320\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mln_f(x)\n\u001b[1;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/jump_models/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/coati/models/encoding/basic_transformer.py:143\u001b[0m, in \u001b[0;36mRotaryBlock.forward\u001b[0;34m(self, x, rotary_embedding)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, rotary_embedding: RotaryEmbedding):\n\u001b[0;32m--> 143\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x), rotary_embedding)\n\u001b[1;32m    144\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlpf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/jump_models/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/coati/models/encoding/basic_transformer.py:118\u001b[0m, in \u001b[0;36mRotarySelfAttention.forward\u001b[0;34m(self, x, rotary_embedding)\u001b[0m\n\u001b[1;32m    116\u001b[0m q, k \u001b[39m=\u001b[39m rotary_embedding\u001b[39m.\u001b[39mrotary_embed(q, k)\n\u001b[1;32m    117\u001b[0m \u001b[39m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m att \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(k\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[1;32m    119\u001b[0m att \u001b[39m=\u001b[39m att\u001b[39m.\u001b[39mmasked_fill(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias[:, :, :T, :T] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    120\u001b[0m att \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(att, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model(smiles).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from s3://terray-public/models/grande_closed.pkl\n",
      "Loading tokenizer may_closedparen from s3://terray-public/models/grande_closed.pkl\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 17.92M Total: 20.36M \n",
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n"
     ]
    }
   ],
   "source": [
    "encoder, tokenizer = load_e3gnn_smiles_clip_e2e(\n",
    "    # model parameters to load.\n",
    "    doc_url=\"s3://terray-public/models/grande_closed.pkl\",\n",
    "    freeze=False,\n",
    "    # device=torch.device(\"cpu\"),\n",
    "    # print_debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[STOP]': 1,\n",
       " '[SMILES]': 2,\n",
       " '[MASK]': 3,\n",
       " '[PREFIX]': 4,\n",
       " '[SUFFIX]': 5,\n",
       " '[MIDDLE]': 6,\n",
       " '[UNK]': 7,\n",
       " '[CLIP]': 8,\n",
       " '[FORMULA]': 9,\n",
       " '[GRAPH]': 10,\n",
       " '[EDGES]': 11,\n",
       " '[EDGE1]': 12,\n",
       " '[EDGEC]': 13,\n",
       " '[EDGE2]': 14,\n",
       " '[EDGE3]': 15,\n",
       " '[SET]': 16,\n",
       " '[ISOMORPHIC]': 17,\n",
       " '[VALID]': 18,\n",
       " '[TRUE]': 19,\n",
       " '[FALSE]': 20,\n",
       " '[geom_drugs]': 21,\n",
       " '[mcule]': 22,\n",
       " '[tspace_real]': 23,\n",
       " '[tensormol]': 24,\n",
       " '[chembl_mols]': 25,\n",
       " '[bbspace]': 26,\n",
       " '[zinc22]': 27,\n",
       " '[tspace_enum]': 28,\n",
       " '[ELM1]': 29,\n",
       " '[ELM2]': 30,\n",
       " '[ELM3]': 31,\n",
       " '[ELM4]': 32,\n",
       " '[ELM5]': 33,\n",
       " '[ELM6]': 34,\n",
       " '[ELM7]': 35,\n",
       " '[ELM8]': 36,\n",
       " '[ELM9]': 37,\n",
       " '[ELM10]': 38,\n",
       " '[ELM11]': 39,\n",
       " '[ELM12]': 40,\n",
       " '[ELM13]': 41,\n",
       " '[ELM14]': 42,\n",
       " '[ELM15]': 43,\n",
       " '[ELM16]': 44,\n",
       " '[ELM17]': 45,\n",
       " '[ELM18]': 46,\n",
       " '[ELM19]': 47,\n",
       " '[ELM20]': 48,\n",
       " '[ELM21]': 49,\n",
       " '[ELM22]': 50,\n",
       " '[ELM23]': 51,\n",
       " '[ELM24]': 52,\n",
       " '[ELM25]': 53,\n",
       " '[ELM26]': 54,\n",
       " '[ELM27]': 55,\n",
       " '[ELM28]': 56,\n",
       " '[ELM29]': 57,\n",
       " '[ELM30]': 58,\n",
       " '[ELM31]': 59,\n",
       " '[ELM32]': 60,\n",
       " '[ELM33]': 61,\n",
       " '[ELM34]': 62,\n",
       " '[ELM35]': 63,\n",
       " '[ELM36]': 64,\n",
       " '[ELM37]': 65,\n",
       " '[ELM38]': 66,\n",
       " '[ELM39]': 67,\n",
       " '[ELM40]': 68,\n",
       " '[ELM41]': 69,\n",
       " '[ELM42]': 70,\n",
       " '[ELM43]': 71,\n",
       " '[ELM44]': 72,\n",
       " '[ELM45]': 73,\n",
       " '[ELM46]': 74,\n",
       " '[ELM47]': 75,\n",
       " '[ELM48]': 76,\n",
       " '[ELM49]': 77,\n",
       " '[ELM50]': 78,\n",
       " '[ELM51]': 79,\n",
       " '[ELM52]': 80,\n",
       " '[ELM53]': 81,\n",
       " '[ELM54]': 82,\n",
       " '[ELM55]': 83,\n",
       " '[ELM56]': 84,\n",
       " '[ELM57]': 85,\n",
       " '[ELM58]': 86,\n",
       " '[ELM59]': 87,\n",
       " '[ELM60]': 88,\n",
       " '[ELM61]': 89,\n",
       " '[ELM62]': 90,\n",
       " '[ELM63]': 91,\n",
       " '[ELM64]': 92,\n",
       " '[ELM65]': 93,\n",
       " '[ELM66]': 94,\n",
       " '[ELM67]': 95,\n",
       " '[ELM68]': 96,\n",
       " '[ELM69]': 97,\n",
       " '[ELM70]': 98,\n",
       " '[ELM71]': 99,\n",
       " '[ELM72]': 100,\n",
       " '[ELM73]': 101,\n",
       " '[ELM74]': 102,\n",
       " '[ELM75]': 103,\n",
       " '[ELM76]': 104,\n",
       " '[ELM77]': 105,\n",
       " '[ELM78]': 106,\n",
       " '[ELM79]': 107,\n",
       " '[ELM80]': 108,\n",
       " '[ELM81]': 109,\n",
       " '[ELM82]': 110,\n",
       " '[ELM83]': 111,\n",
       " '[NUM0]': 112,\n",
       " '[NUM1]': 113,\n",
       " '[NUM2]': 114,\n",
       " '[NUM3]': 115,\n",
       " '[NUM4]': 116,\n",
       " '[NUM5]': 117,\n",
       " '[NUM6]': 118,\n",
       " '[NUM7]': 119,\n",
       " '[NUM8]': 120,\n",
       " '[NUM9]': 121,\n",
       " '[NUM10]': 122,\n",
       " '[NUM11]': 123,\n",
       " '[NUM12]': 124,\n",
       " '[NUM13]': 125,\n",
       " '[NUM14]': 126,\n",
       " '[NUM15]': 127,\n",
       " '[NUM16]': 128,\n",
       " '[NUM17]': 129,\n",
       " '[NUM18]': 130,\n",
       " '[NUM19]': 131,\n",
       " '[NUM20]': 132,\n",
       " '[NUM21]': 133,\n",
       " '[NUM22]': 134,\n",
       " '[NUM23]': 135,\n",
       " '[NUM24]': 136,\n",
       " '[NUM25]': 137,\n",
       " '[NUM26]': 138,\n",
       " '[NUM27]': 139,\n",
       " '[NUM28]': 140,\n",
       " '[NUM29]': 141,\n",
       " '[NUM30]': 142,\n",
       " '[NUM31]': 143,\n",
       " '[NUM32]': 144,\n",
       " '[NUM33]': 145,\n",
       " '[NUM34]': 146,\n",
       " '[NUM35]': 147,\n",
       " '[NUM36]': 148,\n",
       " '[NUM37]': 149,\n",
       " '[NUM38]': 150,\n",
       " '[NUM39]': 151,\n",
       " '[NUM40]': 152,\n",
       " '[NUM41]': 153,\n",
       " '[NUM42]': 154,\n",
       " '[NUM43]': 155,\n",
       " '[NUM44]': 156,\n",
       " '[NUM45]': 157,\n",
       " '[NUM46]': 158,\n",
       " '[NUM47]': 159,\n",
       " '[NUM48]': 160,\n",
       " '[NUM49]': 161,\n",
       " '[NUM50]': 162,\n",
       " '[NUM51]': 163,\n",
       " '[NUM52]': 164,\n",
       " '[NUM53]': 165,\n",
       " '[NUM54]': 166,\n",
       " '[NUM55]': 167,\n",
       " '[NUM56]': 168,\n",
       " '[NUM57]': 169,\n",
       " '[NUM58]': 170,\n",
       " '[NUM59]': 171,\n",
       " '[NUM60]': 172,\n",
       " '[NUM61]': 173,\n",
       " '[NUM62]': 174,\n",
       " '[NUM63]': 175,\n",
       " '[NUM64]': 176,\n",
       " '[NUM65]': 177,\n",
       " '[NUM66]': 178,\n",
       " '[NUM67]': 179,\n",
       " '[NUM68]': 180,\n",
       " '[NUM69]': 181,\n",
       " '[NUM70]': 182,\n",
       " '[NUM71]': 183,\n",
       " '[NUM72]': 184,\n",
       " '[NUM73]': 185,\n",
       " '[NUM74]': 186,\n",
       " '[NUM75]': 187,\n",
       " '[NUM76]': 188,\n",
       " '[NUM77]': 189,\n",
       " '[NUM78]': 190,\n",
       " '[NUM79]': 191,\n",
       " '[NUM80]': 192,\n",
       " '[NUM81]': 193,\n",
       " '[NUM82]': 194,\n",
       " '[NUM83]': 195,\n",
       " '[NUM84]': 196,\n",
       " '[NUM85]': 197,\n",
       " '[NUM86]': 198,\n",
       " '[NUM87]': 199,\n",
       " '[NUM88]': 200,\n",
       " '[NUM89]': 201,\n",
       " '[NUM90]': 202,\n",
       " '[NUM91]': 203,\n",
       " '[NUM92]': 204,\n",
       " '[NUM93]': 205,\n",
       " '[NUM94]': 206,\n",
       " '[NUM95]': 207,\n",
       " '[NUM96]': 208,\n",
       " '[NUM97]': 209,\n",
       " '[NUM98]': 210,\n",
       " '[NUM99]': 211,\n",
       " '[NUM100]': 212,\n",
       " '[NUM101]': 213,\n",
       " '[NUM102]': 214,\n",
       " '[NUM103]': 215,\n",
       " '[NUM104]': 216,\n",
       " '[NUM105]': 217,\n",
       " '[NUM106]': 218,\n",
       " '[NUM107]': 219,\n",
       " '[NUM108]': 220,\n",
       " '[NUM109]': 221,\n",
       " '[NUM110]': 222,\n",
       " '[NUM111]': 223,\n",
       " '[NUM112]': 224,\n",
       " '[NUM113]': 225,\n",
       " '[NUM114]': 226,\n",
       " '[NUM115]': 227,\n",
       " '[NUM116]': 228,\n",
       " '[NUM117]': 229,\n",
       " '[NUM118]': 230,\n",
       " '[NUM119]': 231,\n",
       " '[NUM120]': 232,\n",
       " '[NUM121]': 233,\n",
       " '[NUM122]': 234,\n",
       " '[NUM123]': 235,\n",
       " '[NUM124]': 236,\n",
       " '[NUM125]': 237,\n",
       " '[NUM126]': 238,\n",
       " '[NUM127]': 239,\n",
       " '[NUM128]': 240,\n",
       " '[NUM129]': 241,\n",
       " '[NUM130]': 242,\n",
       " '[NUM131]': 243,\n",
       " '[NUM132]': 244,\n",
       " '[NUM133]': 245,\n",
       " '[NUM134]': 246,\n",
       " '[NUM135]': 247,\n",
       " '[NUM136]': 248,\n",
       " '[NUM137]': 249,\n",
       " '[NUM138]': 250,\n",
       " '[NUM139]': 251,\n",
       " '[NUM140]': 252,\n",
       " '[NUM141]': 253,\n",
       " '[NUM142]': 254,\n",
       " '[NUM143]': 255,\n",
       " '[NUM144]': 256,\n",
       " '[NUM145]': 257,\n",
       " '[NUM146]': 258,\n",
       " '[NUM147]': 259,\n",
       " '[NUM148]': 260,\n",
       " '[NUM149]': 261,\n",
       " '[NUM150]': 262,\n",
       " '[ELM7][NUM5]': 263,\n",
       " '[ELM8][NUM4]': 264,\n",
       " '[ELM9][NUM1]': 265,\n",
       " '[ELM7][NUM3]': 266,\n",
       " '[ELM8][NUM3]': 267,\n",
       " '[ELM8][NUM2]': 268,\n",
       " '[ELM7][NUM6]': 269,\n",
       " '[ELM7][NUM1]': 270,\n",
       " '[SET][mcule]': 271,\n",
       " '[ELM7][NUM7]': 272,\n",
       " '[ELM7][NUM4]': 273,\n",
       " '[ELM9][NUM3]': 274,\n",
       " '[ELM8][NUM6]': 275,\n",
       " '[ELM8][NUM5]': 276,\n",
       " '[ELM8][NUM1]': 277,\n",
       " '[NUM1][ELM9]': 278,\n",
       " '[ELM7][NUM2]': 279,\n",
       " '[NUM0][NUM1]': 280,\n",
       " '[NUM1][NUM2]': 281,\n",
       " '[NUM2][NUM3]': 282,\n",
       " '[NUM3][NUM4]': 283,\n",
       " '[NUM4][NUM5]': 284,\n",
       " '[NUM6][NUM7]': 285,\n",
       " '[NUM7][NUM8]': 286,\n",
       " '[NUM8][NUM9]': 287,\n",
       " '[ELM9][NUM4]': 288,\n",
       " '[ELM9][NUM5]': 289,\n",
       " '[ELM9][NUM2]': 290,\n",
       " '[ELM7][NUM9]': 291,\n",
       " '[NUM5][NUM6]': 292,\n",
       " '[ELM8][NUM7]': 293,\n",
       " '[ELM9][NUM6]': 294,\n",
       " '[ELM7][NUM8]': 295,\n",
       " '[NUM3][NUM5]': 296,\n",
       " '[ELM8][NUM8]': 297,\n",
       " '[NUM6][NUM8]': 298,\n",
       " '[NUM2][NUM4]': 299,\n",
       " '[NUM1][NUM3]': 300,\n",
       " '[NUM7][NUM9]': 301,\n",
       " '[NUM2][NUM7]': 302,\n",
       " '[NUM4][NUM6]': 303,\n",
       " '[NUM5][NUM7]': 304,\n",
       " '[NUM1][NUM4]': 305,\n",
       " '[NUM1][NUM5]': 306,\n",
       " '[NUM3][NUM7]': 307,\n",
       " '[NUM2][NUM5]': 308,\n",
       " '[NUM6][NUM9]': 309,\n",
       " '[NUM1][NUM6]': 310,\n",
       " '[NUM1][NUM7]': 311,\n",
       " '[NUM2][NUM8]': 312,\n",
       " '[NUM1][NUM9]': 313,\n",
       " '[NUM3][NUM8]': 314,\n",
       " '[NUM4][NUM9]': 315,\n",
       " '[NUM5][NUM9]': 316,\n",
       " '[ELM9][NUM7]': 317,\n",
       " '[NUM4][NUM8]': 318,\n",
       " '[NUM3][NUM9]': 319,\n",
       " '[NUM5][NUM8]': 320,\n",
       " '[NUM2][NUM6]': 321,\n",
       " '[NUM4][NUM7]': 322,\n",
       " '[NUM1][NUM8]': 323,\n",
       " '[ELM8][NUM9]': 324,\n",
       " '[ELM9][NUM8]': 325,\n",
       " '[ELM6][NUM4]': 326,\n",
       " '[ELM6][NUM7]': 327,\n",
       " '[ELM6][NUM8]': 328,\n",
       " '[ELM6][NUM6]': 329,\n",
       " '[NUM3][NUM6]': 330,\n",
       " '[ELM9][NUM9]': 331,\n",
       " '[NUM23][ELM6]': 332,\n",
       " '[NUM17][ELM7]': 333,\n",
       " '[ELM16][NUM1]': 334,\n",
       " '[ELM17][NUM1]': 335,\n",
       " '[NUM19][ELM6]': 336,\n",
       " '[NUM21][ELM7]': 337,\n",
       " '[ELM17][NUM2]': 338,\n",
       " '[NUM22][ELM6]': 339,\n",
       " '[NUM20][ELM7]': 340,\n",
       " '[SET][zinc22]': 341,\n",
       " '[NUM19][ELM7]': 342,\n",
       " '[ELM35][NUM1]': 343,\n",
       " '[NUM21][ELM6]': 344,\n",
       " '[NUM18][ELM6]': 345,\n",
       " '[NUM9][NUM10]': 346,\n",
       " '[ELM16][NUM2]': 347,\n",
       " '[NUM18][ELM7]': 348,\n",
       " '[NUM22][ELM7]': 349,\n",
       " '[NUM16][ELM6]': 350,\n",
       " '[NUM17][ELM6]': 351,\n",
       " '[ELM15][NUM1]': 352,\n",
       " '[ELM17][NUM3]': 353,\n",
       " '[NUM20][ELM6]': 354,\n",
       " '[ELM14][NUM1]': 355,\n",
       " '[ELM16][NUM3]': 356,\n",
       " '[NUM8][NUM10]': 357,\n",
       " '[ELM53][NUM1]': 358,\n",
       " '[NUM19][ELM8]': 359,\n",
       " '[NUM22][ELM8]': 360,\n",
       " '[NUM6][NUM10]': 361,\n",
       " '[NUM7][NUM13]': 362,\n",
       " '[NUM5][NUM15]': 363,\n",
       " '[NUM8][NUM12]': 364,\n",
       " '[NUM8][NUM16]': 365,\n",
       " '[NUM8][NUM20]': 366,\n",
       " '[NUM5][NUM21]': 367,\n",
       " '[NUM1][NUM21]': 368,\n",
       " '[NUM9][NUM15]': 369,\n",
       " '[NUM20][ELM8]': 370,\n",
       " '[NUM2][NUM15]': 371,\n",
       " '[NUM9][NUM14]': 372,\n",
       " '[NUM1][NUM12]': 373,\n",
       " '[NUM6][NUM11]': 374,\n",
       " '[NUM4][NUM20]': 375,\n",
       " '[NUM4][NUM18]': 376,\n",
       " '[NUM1][NUM19]': 377,\n",
       " '[NUM6][NUM16]': 378,\n",
       " '[NUM9][NUM11]': 379,\n",
       " '[NUM7][NUM12]': 380,\n",
       " '[NUM18][ELM8]': 381,\n",
       " '[NUM7][NUM22]': 382,\n",
       " '[NUM4][NUM23]': 383,\n",
       " '[NUM6][NUM19]': 384,\n",
       " '[NUM3][NUM19]': 385,\n",
       " '[NUM1][NUM17]': 386,\n",
       " '[NUM2][NUM21]': 387,\n",
       " '[NUM9][NUM16]': 388,\n",
       " '[NUM7][NUM14]': 389,\n",
       " '[NUM7][NUM11]': 390,\n",
       " '[NUM1][NUM15]': 391,\n",
       " '[NUM5][NUM11]': 392,\n",
       " '[NUM6][NUM20]': 393,\n",
       " '[NUM5][NUM17]': 394,\n",
       " '[NUM5][NUM16]': 395,\n",
       " '[NUM7][NUM10]': 396,\n",
       " '[NUM5][NUM19]': 397,\n",
       " '[NUM9][NUM21]': 398,\n",
       " '[NUM6][NUM22]': 399,\n",
       " '[NUM2][NUM19]': 400,\n",
       " '[NUM3][NUM14]': 401,\n",
       " '[NUM2][NUM22]': 402,\n",
       " '[NUM5][NUM14]': 403,\n",
       " '[NUM5][NUM22]': 404,\n",
       " '[NUM8][NUM22]': 405,\n",
       " '[NUM9][NUM12]': 406,\n",
       " '[NUM8][NUM19]': 407,\n",
       " '[NUM4][NUM22]': 408,\n",
       " '[NUM5][NUM20]': 409,\n",
       " '[NUM8][NUM14]': 410,\n",
       " '[NUM1][NUM13]': 411,\n",
       " '[NUM1][NUM14]': 412,\n",
       " '[NUM7][NUM15]': 413,\n",
       " '[NUM8][NUM13]': 414,\n",
       " '[NUM7][NUM18]': 415,\n",
       " '[NUM9][NUM18]': 416,\n",
       " '[NUM6][NUM18]': 417,\n",
       " '[NUM1][NUM11]': 418,\n",
       " '[NUM6][NUM13]': 419,\n",
       " '[NUM9][NUM19]': 420,\n",
       " '[NUM5][NUM13]': 421,\n",
       " '[NUM4][NUM14]': 422,\n",
       " '[NUM4][NUM11]': 423,\n",
       " '[NUM5][NUM10]': 424,\n",
       " '[NUM6][NUM21]': 425,\n",
       " '[NUM1][NUM10]': 426,\n",
       " '[NUM3][NUM15]': 427,\n",
       " '[NUM2][NUM17]': 428,\n",
       " '[NUM5][NUM18]': 429,\n",
       " '[NUM1][NUM16]': 430,\n",
       " '[NUM4][NUM21]': 431,\n",
       " '[NUM1][NUM22]': 432,\n",
       " '[NUM9][NUM13]': 433,\n",
       " '[NUM1][NUM18]': 434,\n",
       " '[NUM7][NUM21]': 435,\n",
       " '[NUM8][NUM15]': 436,\n",
       " '[NUM4][NUM16]': 437,\n",
       " '[NUM7][NUM17]': 438,\n",
       " '[NUM8][NUM17]': 439,\n",
       " '[NUM6][NUM15]': 440,\n",
       " '[NUM4][NUM13]': 441,\n",
       " '[NUM6][NUM12]': 442,\n",
       " '[NUM7][NUM20]': 443,\n",
       " '[NUM9][NUM20]': 444,\n",
       " '[NUM4][NUM19]': 445,\n",
       " '[NUM8][NUM21]': 446,\n",
       " '[NUM8][NUM11]': 447,\n",
       " '[NUM2][NUM20]': 448,\n",
       " '[NUM6][NUM14]': 449,\n",
       " '[NUM1][NUM20]': 450,\n",
       " '[NUM5][NUM12]': 451,\n",
       " '[NUM4][NUM15]': 452,\n",
       " '[NUM8][NUM18]': 453,\n",
       " '[NUM2][NUM18]': 454,\n",
       " '[NUM4][NUM10]': 455,\n",
       " '[NUM3][NUM20]': 456,\n",
       " '[NUM2][NUM14]': 457,\n",
       " '[NUM7][NUM16]': 458,\n",
       " '[NUM7][NUM19]': 459,\n",
       " '[NUM9][NUM17]': 460,\n",
       " '[NUM4][NUM17]': 461,\n",
       " '[NUM3][NUM18]': 462,\n",
       " '[NUM4][NUM12]': 463,\n",
       " '[NUM6][NUM17]': 464,\n",
       " '[ELM35][NUM2]': 465,\n",
       " '[ELM8][NUM13]': 466,\n",
       " '[ELM6][NUM14]': 467,\n",
       " '[NUM15][ELM8]': 468,\n",
       " '[ELM6][NUM11]': 469,\n",
       " '[ELM6][NUM10]': 470,\n",
       " '[ELM6][NUM13]': 471,\n",
       " '[NUM13][ELM8]': 472,\n",
       " '[NUM13][ELM7]': 473,\n",
       " '[ELM16][NUM7]': 474,\n",
       " '[NUM10][ELM8]': 475,\n",
       " '[NUM16][ELM7]': 476,\n",
       " '[ELM8][NUM10]': 477,\n",
       " '[ELM7][NUM14]': 478,\n",
       " '[NUM16][ELM8]': 479,\n",
       " '[NUM14][ELM7]': 480,\n",
       " '[ELM6][NUM12]': 481,\n",
       " '[ELM9][NUM14]': 482,\n",
       " '[NUM18][ELM9]': 483,\n",
       " '[ELM6][NUM16]': 484,\n",
       " '[NUM11][ELM7]': 485,\n",
       " '[NUM11][ELM8]': 486,\n",
       " '[ELM8][NUM14]': 487,\n",
       " '[NUM14][ELM8]': 488,\n",
       " '[NUM10][ELM7]': 489,\n",
       " '[NUM15][ELM7]': 490,\n",
       " '[NUM16][ELM9]': 491,\n",
       " '[NUM20][ELM9]': 492,\n",
       " '[ELM16][NUM8]': 493,\n",
       " '[NUM17][ELM8]': 494,\n",
       " '[NUM17][ELM9]': 495,\n",
       " '[NUM19][ELM9]': 496,\n",
       " '[ELM8][NUM15]': 497,\n",
       " '[NUM21][ELM8]': 498,\n",
       " '[ELM9][NUM12]': 499,\n",
       " '[NUM3][NUM13]': 500,\n",
       " '[NUM3][NUM21]': 501,\n",
       " '[NUM3][NUM17]': 502,\n",
       " '[NUM2][NUM16]': 503,\n",
       " '[ELM1][NUM50]': 504,\n",
       " '[NUM3][NUM11]': 505,\n",
       " '[NUM21][ELM9]': 506,\n",
       " '[NUM15][ELM6]': 507,\n",
       " '[NUM3][NUM12]': 508,\n",
       " '[NUM2][NUM10]': 509,\n",
       " '[NUM3][NUM16]': 510,\n",
       " '[NUM3][NUM10]': 511,\n",
       " '[NUM9][NUM22]': 512,\n",
       " '[NUM2][NUM23]': 513,\n",
       " '[NUM2][NUM12]': 514,\n",
       " '[NUM22][ELM9]': 515,\n",
       " '[NUM2][NUM13]': 516,\n",
       " '[NUM12][NUM13]': 517,\n",
       " '[NUM13][NUM14]': 518,\n",
       " '[NUM10][NUM11]': 519,\n",
       " '[NUM11][NUM12]': 520,\n",
       " '[NUM14][NUM15]': 521,\n",
       " '[NUM15][NUM16]': 522,\n",
       " '[NUM15][NUM17]': 523,\n",
       " '[NUM17][NUM18]': 524,\n",
       " '[NUM18][NUM19]': 525,\n",
       " '[NUM19][NUM20]': 526,\n",
       " '[NUM18][NUM21]': 527,\n",
       " '[NUM21][NUM22]': 528,\n",
       " '[NUM13][NUM21]': 529,\n",
       " '[NUM18][NUM20]': 530,\n",
       " '[NUM22][NUM23]': 531,\n",
       " '[NUM14][NUM20]': 532,\n",
       " '[NUM16][NUM17]': 533,\n",
       " '[NUM14][NUM16]': 534,\n",
       " '[NUM16][NUM18]': 535,\n",
       " '[NUM20][NUM21]': 536,\n",
       " '[NUM18][NUM22]': 537,\n",
       " '[NUM12][NUM14]': 538,\n",
       " '[NUM10][NUM12]': 539,\n",
       " '[NUM13][NUM19]': 540,\n",
       " '[NUM16][NUM20]': 541,\n",
       " '[NUM11][NUM13]': 542,\n",
       " '[NUM21][NUM23]': 543,\n",
       " '[NUM12][NUM20]': 544,\n",
       " '[NUM13][NUM15]': 545,\n",
       " '[NUM12][NUM16]': 546,\n",
       " '[NUM12][NUM17]': 547,\n",
       " '[NUM15][NUM21]': 548,\n",
       " '[NUM19][NUM21]': 549,\n",
       " '[NUM14][NUM21]': 550,\n",
       " '[NUM14][NUM19]': 551,\n",
       " '[NUM13][NUM17]': 552,\n",
       " '[NUM15][NUM19]': 553,\n",
       " '[NUM10][NUM15]': 554,\n",
       " '[NUM10][NUM13]': 555,\n",
       " '[NUM10][NUM17]': 556,\n",
       " '[NUM12][NUM15]': 557,\n",
       " '[NUM16][NUM19]': 558,\n",
       " '[NUM11][NUM17]': 559,\n",
       " '[NUM11][NUM20]': 560,\n",
       " '[NUM11][NUM21]': 561,\n",
       " '[NUM15][NUM20]': 562,\n",
       " '[NUM17][NUM19]': 563,\n",
       " '[NUM12][NUM19]': 564,\n",
       " '[NUM11][NUM14]': 565,\n",
       " '[NUM10][NUM18]': 566,\n",
       " '[NUM17][NUM21]': 567,\n",
       " '[NUM17][NUM22]': 568,\n",
       " '[NUM16][NUM22]': 569,\n",
       " '[NUM15][NUM18]': 570,\n",
       " '[NUM11][NUM18]': 571,\n",
       " '[NUM19][NUM22]': 572,\n",
       " '[NUM11][NUM16]': 573,\n",
       " '[NUM12][NUM18]': 574,\n",
       " '[NUM16][NUM21]': 575,\n",
       " '[NUM13][NUM18]': 576,\n",
       " '[NUM10][NUM21]': 577,\n",
       " '[NUM10][NUM19]': 578,\n",
       " '[NUM13][NUM16]': 579,\n",
       " '[NUM10][NUM16]': 580,\n",
       " '[NUM10][NUM20]': 581,\n",
       " '[NUM14][NUM17]': 582,\n",
       " '[NUM11][NUM19]': 583,\n",
       " '[NUM10][NUM14]': 584,\n",
       " '[NUM14][NUM18]': 585,\n",
       " '[NUM20][NUM22]': 586,\n",
       " '[NUM12][NUM21]': 587,\n",
       " '[NUM17][NUM20]': 588,\n",
       " '[NUM11][NUM15]': 589,\n",
       " '[NUM13][NUM20]': 590,\n",
       " '[NUM17][ELM16]': 591,\n",
       " '[NUM18][ELM16]': 592,\n",
       " '[NUM19][ELM16]': 593,\n",
       " '[NUM17][ELM17]': 594,\n",
       " '[NUM18][NUM23]': 595,\n",
       " '[NUM14][NUM22]': 596,\n",
       " '[NUM10][NUM22]': 597,\n",
       " '[NUM13][NUM22]': 598,\n",
       " '[NUM11][NUM22]': 599,\n",
       " '[NUM15][NUM22]': 600,\n",
       " '[SET][geom_drugs]': 601,\n",
       " '[SET][tspace_enum]': 602,\n",
       " '[ELM1][NUM7][ELM6]': 603,\n",
       " '[ELM1][NUM6][ELM6]': 604,\n",
       " '[SET][tspace_real]': 605,\n",
       " '[ELM6][NUM9][ELM7]': 606,\n",
       " '[ELM7][NUM9][ELM6]': 607,\n",
       " '[NUM3][ELM8][NUM4]': 608,\n",
       " '[NUM4][ELM8][NUM5]': 609,\n",
       " '[NUM2][ELM8][NUM3]': 610,\n",
       " '[NUM1][ELM8][NUM2]': 611,\n",
       " '[NUM5][ELM8][NUM2]': 612,\n",
       " '[NUM4][ELM8][NUM2]': 613,\n",
       " '[ELM1][NUM9][ELM6]': 614,\n",
       " '[NUM3][ELM8][NUM3]': 615,\n",
       " '[NUM4][ELM8][NUM3]': 616,\n",
       " '[NUM3][ELM8][NUM2]': 617,\n",
       " '[ELM1][NUM8][ELM6]': 618,\n",
       " '[NUM2][ELM8][NUM2]': 619,\n",
       " '[NUM3][ELM8][NUM5]': 620,\n",
       " '[NUM1][ELM8][NUM3]': 621,\n",
       " '[NUM1][ELM8][NUM1]': 622,\n",
       " '[NUM5][ELM8][NUM3]': 623,\n",
       " '[NUM2][ELM8][NUM5]': 624,\n",
       " '[NUM4][ELM8][NUM4]': 625,\n",
       " '[NUM2][ELM8][NUM4]': 626,\n",
       " '[NUM6][ELM8][NUM2]': 627,\n",
       " '[NUM5][ELM8][NUM1]': 628,\n",
       " '[NUM2][ELM8][NUM1]': 629,\n",
       " '[NUM4][ELM8][NUM1]': 630,\n",
       " '[NUM3][ELM8][NUM1]': 631,\n",
       " '[NUM0][ELM9][NUM1]': 632,\n",
       " '[ELM6][NUM9][ELM9]': 633,\n",
       " '[ELM7][NUM10][ELM8]': 634,\n",
       " '[ELM1][NUM11][ELM6]': 635,\n",
       " '[ELM1][NUM38][ELM6]': 636,\n",
       " '[ELM1][NUM21][ELM6]': 637,\n",
       " '[ELM1][NUM12][ELM6]': 638,\n",
       " '[ELM1][NUM36][ELM6]': 639,\n",
       " '[ELM1][NUM44][ELM6]': 640,\n",
       " '[ELM1][NUM10][ELM6]': 641,\n",
       " '[ELM1][NUM20][ELM6]': 642,\n",
       " '[ELM1][NUM37][ELM6]': 643,\n",
       " '[ELM1][NUM39][ELM6]': 644,\n",
       " '[ELM1][NUM13][ELM6]': 645,\n",
       " '[ELM1][NUM15][ELM6]': 646,\n",
       " '[ELM7][NUM15][ELM6]': 647,\n",
       " '[ELM1][NUM24][ELM6]': 648,\n",
       " '[ELM8][NUM16][ELM6]': 649,\n",
       " '[ELM1][NUM19][ELM6]': 650,\n",
       " '[ELM1][NUM16][ELM6]': 651,\n",
       " '[ELM1][NUM30][ELM6]': 652,\n",
       " '[ELM1][NUM17][ELM6]': 653,\n",
       " '[ELM1][NUM40][ELM6]': 654,\n",
       " '[ELM1][NUM22][ELM6]': 655,\n",
       " '[ELM1][NUM27][ELM6]': 656,\n",
       " '[ELM1][NUM41][ELM6]': 657,\n",
       " '[ELM1][NUM23][ELM6]': 658,\n",
       " '[ELM1][NUM29][ELM6]': 659,\n",
       " '[ELM7][NUM16][ELM6]': 660,\n",
       " '[ELM8][NUM17][ELM6]': 661,\n",
       " '[ELM1][NUM35][ELM6]': 662,\n",
       " '[ELM1][NUM26][ELM6]': 663,\n",
       " '[ELM1][NUM25][ELM6]': 664,\n",
       " '[ELM1][NUM42][ELM6]': 665,\n",
       " '[ELM7][NUM17][ELM6]': 666,\n",
       " '[ELM1][NUM32][ELM6]': 667,\n",
       " '[NUM3][ELM16][NUM1]': 668,\n",
       " '[ELM1][NUM18][ELM6]': 669,\n",
       " '[ELM1][NUM34][ELM6]': 670,\n",
       " '[ELM1][NUM14][ELM6]': 671,\n",
       " '[ELM1][NUM28][ELM6]': 672,\n",
       " '[ELM1][NUM48][ELM6]': 673,\n",
       " '[ELM6][NUM15][ELM9]': 674,\n",
       " '[ELM1][NUM46][ELM6]': 675,\n",
       " '[ELM1][NUM47][ELM6]': 676,\n",
       " '[NUM1][ELM16][NUM1]': 677,\n",
       " '[ELM1][NUM43][ELM6]': 678,\n",
       " '[ELM1][NUM31][ELM6]': 679,\n",
       " '[NUM2][ELM16][NUM1]': 680,\n",
       " '[ELM1][NUM45][ELM6]': 681,\n",
       " '[NUM4][ELM16][NUM1]': 682,\n",
       " '[NUM10][ELM6][NUM11]': 683,\n",
       " '[NUM11][ELM6][NUM12]': 684,\n",
       " '[NUM13][ELM6][NUM14]': 685,\n",
       " '[ELM7][NUM3][ELM8][NUM2]': 686,\n",
       " '[ELM7][NUM5][ELM8][NUM2]': 687,\n",
       " '[ELM7][NUM1][ELM8][NUM4]': 688,\n",
       " '[ELM7][NUM2][ELM8][NUM2]': 689,\n",
       " '[ELM7][NUM4][ELM8][NUM2]': 690,\n",
       " '[ELM7][NUM1][ELM8][NUM3]': 691,\n",
       " '[ELM7][NUM5][ELM8][NUM3]': 692,\n",
       " '[ELM7][NUM5][ELM8][NUM1]': 693,\n",
       " '[ELM7][NUM3][ELM8][NUM7]': 694,\n",
       " '[ELM7][NUM3][ELM8][NUM1]': 695,\n",
       " '[ELM7][NUM4][ELM8][NUM3]': 696,\n",
       " '[ELM7][NUM4][ELM8][NUM1]': 697,\n",
       " '[ELM7][NUM3][ELM8][NUM3]': 698,\n",
       " '[ELM7][NUM2][ELM8][NUM3]': 699,\n",
       " '[ELM7][NUM2][ELM8][NUM4]': 700,\n",
       " '[ELM7][NUM4][ELM8][NUM5]': 701,\n",
       " '[ELM7][NUM6][ELM8][NUM2]': 702,\n",
       " '[ELM7][NUM4][ELM8][NUM4]': 703,\n",
       " '[ELM7][NUM6][ELM8][NUM4]': 704,\n",
       " '[ELM7][NUM3][ELM8][NUM4]': 705,\n",
       " '[ELM7][NUM2][ELM8][NUM1]': 706,\n",
       " '[ELM7][NUM1][ELM8][NUM2]': 707,\n",
       " '[ELM7][NUM8][ELM8][NUM3]': 708,\n",
       " '[ELM7][NUM1][ELM8][NUM1]': 709,\n",
       " '[ELM7][NUM6][ELM8][NUM3]': 710,\n",
       " '[ELM7][NUM7][ELM8][NUM2]': 711,\n",
       " '[ELM7][NUM6][ELM8][NUM1]': 712,\n",
       " '[ELM7][NUM2][ELM8][NUM6]': 713,\n",
       " '[ELM7][NUM6][ELM8][NUM5]': 714,\n",
       " '[ELM7][NUM3][ELM8][NUM5]': 715,\n",
       " '[ELM7][NUM5][ELM8][NUM5]': 716,\n",
       " '[ELM7][NUM5][ELM8][NUM4]': 717,\n",
       " '[ELM7][NUM7][ELM8][NUM3]': 718,\n",
       " '[ELM7][NUM5][ELM8][NUM6]': 719,\n",
       " '[ELM7][NUM2][ELM8][NUM5]': 720,\n",
       " '[ELM7][NUM7][ELM8][NUM4]': 721,\n",
       " '[ELM7][NUM1][ELM8][NUM6]': 722,\n",
       " '[ELM7][NUM1][ELM8][NUM5]': 723,\n",
       " '[ELM7][NUM3][ELM8][NUM6]': 724,\n",
       " '[ELM7][NUM7][ELM8][NUM5]': 725,\n",
       " '[ELM7][NUM7][ELM8][NUM1]': 726,\n",
       " '[ELM7][NUM8][ELM8][NUM2]': 727,\n",
       " '[ELM7][NUM6][ELM8][NUM6]': 728,\n",
       " '[ELM7][NUM6][ELM8][NUM7]': 729,\n",
       " '[ELM7][NUM2][ELM8][NUM7]': 730,\n",
       " '[ELM7][NUM5][ELM8][NUM7]': 731,\n",
       " '[ELM7][NUM4][ELM8][NUM6]': 732,\n",
       " '[ELM7][NUM4][ELM8][NUM7]': 733,\n",
       " '[ELM7][NUM8][ELM8][NUM1]': 734,\n",
       " '[ELM7][NUM8][ELM8][NUM4]': 735,\n",
       " '[ELM7][NUM7][ELM8][NUM6]': 736,\n",
       " '[ELM7][NUM9][ELM8][NUM3]': 737,\n",
       " '[ELM7][NUM8][ELM8][NUM5]': 738,\n",
       " '[ELM8][NUM8][ELM6][NUM9]': 739,\n",
       " '[ELM7][NUM7][ELM6][NUM8]': 740,\n",
       " '[ELM6][NUM6][ELM6][NUM7]': 741,\n",
       " '[ELM7][NUM8][ELM7][NUM9]': 742,\n",
       " '[ELM7][NUM1][ELM8][NUM7]': 743,\n",
       " '[ELM7][NUM6][ELM7][NUM7]': 744,\n",
       " '[NUM3][ELM8][NUM1][ELM9]': 745,\n",
       " '[ELM8][NUM6][ELM6][NUM7]': 746,\n",
       " '[ELM8][NUM7][ELM6][NUM8]': 747,\n",
       " '[NUM8][ELM8][NUM9][ELM6]': 748,\n",
       " '[NUM2][ELM8][NUM2][ELM9]': 749,\n",
       " '[ELM6][NUM4][ELM6][NUM5]': 750,\n",
       " '[NUM2][ELM8][NUM1][ELM9]': 751,\n",
       " '[ELM7][NUM9][ELM8][NUM4]': 752,\n",
       " '[ELM9][NUM1][ELM16][NUM1]': 753,\n",
       " '[ELM9][NUM3][ELM16][NUM1]': 754,\n",
       " '[ELM9][NUM2][ELM16][NUM1]': 755,\n",
       " '[ELM9][NUM1][ELM17][NUM1]': 756,\n",
       " '[ELM9][NUM4][ELM16][NUM1]': 757,\n",
       " '[ELM9][NUM6][ELM16][NUM1]': 758,\n",
       " '[NUM11][ELM7][NUM2][ELM8]': 759,\n",
       " '[ELM7][NUM3][ELM16][NUM1]': 760,\n",
       " '[ELM9][NUM3][ELM17][NUM1]': 761,\n",
       " '[ELM9][NUM2][ELM17][NUM1]': 762,\n",
       " '[ELM35][NUM1][SET][mcule]': 763,\n",
       " '[ELM1][NUM13][ELM6][NUM14]': 764,\n",
       " '[ELM1][NUM18][ELM6][NUM24]': 765,\n",
       " '[ELM1][NUM28][ELM6][NUM21]': 766,\n",
       " '[ELM1][NUM30][ELM6][NUM28]': 767,\n",
       " '[ELM16][NUM1][ELM17][NUM1]': 768,\n",
       " '[NUM17][ELM6][NUM18][ELM6]': 769,\n",
       " '[NUM20][ELM6][NUM21][ELM6]': 770,\n",
       " '[ELM1][NUM19][ELM6][NUM15]': 771,\n",
       " '[ELM1][NUM35][ELM6][NUM29]': 772,\n",
       " '[ELM1][NUM29][ELM6][NUM25]': 773,\n",
       " '[NUM17][ELM7][NUM18][ELM6]': 774,\n",
       " '[NUM18][ELM6][NUM19][ELM6]': 775,\n",
       " '[NUM19][ELM6][NUM20][ELM6]': 776,\n",
       " '[NUM21][ELM6][NUM22][ELM6]': 777,\n",
       " '[ELM1][NUM12][ELM6][NUM16]': 778,\n",
       " '[ELM1][NUM21][ELM6][NUM20]': 779,\n",
       " '[NUM18][ELM7][NUM19][ELM6]': 780,\n",
       " '[ELM1][NUM14][ELM6][NUM14]': 781,\n",
       " '[ELM1][NUM17][ELM6][NUM19]': 782,\n",
       " '[ELM1][NUM30][ELM6][NUM23]': 783,\n",
       " '[ELM1][NUM18][ELM6][NUM19]': 784,\n",
       " '[ELM1][NUM30][ELM6][NUM22]': 785,\n",
       " '[ELM1][NUM27][ELM6][NUM24]': 786,\n",
       " '[ELM1][NUM26][ELM6][NUM28]': 787,\n",
       " '[ELM1][NUM24][ELM6][NUM26]': 788,\n",
       " '[ELM1][NUM17][ELM6][NUM17]': 789,\n",
       " '[ELM1][NUM22][ELM6][NUM23]': 790,\n",
       " '[ELM1][NUM34][ELM6][NUM22]': 791,\n",
       " '[ELM1][NUM26][ELM6][NUM27]': 792,\n",
       " '[ELM1][NUM16][ELM6][NUM18]': 793,\n",
       " '[ELM1][NUM23][ELM6][NUM19]': 794,\n",
       " '[ELM1][NUM29][ELM6][NUM22]': 795,\n",
       " '[ELM1][NUM35][ELM6][NUM24]': 796,\n",
       " '[ELM1][NUM20][ELM6][NUM19]': 797,\n",
       " '[ELM1][NUM21][ELM6][NUM16]': 798,\n",
       " '[ELM1][NUM24][ELM6][NUM22]': 799,\n",
       " '[ELM16][NUM2][ELM17][NUM1]': 800,\n",
       " '[ELM1][NUM27][ELM6][NUM21]': 801,\n",
       " '[ELM1][NUM24][ELM6][NUM23]': 802,\n",
       " '[ELM1][NUM29][ELM6][NUM26]': 803,\n",
       " '[ELM1][NUM17][ELM6][NUM22]': 804,\n",
       " '[ELM1][NUM21][ELM6][NUM21]': 805,\n",
       " '[ELM1][NUM23][ELM6][NUM20]': 806,\n",
       " '[ELM1][NUM17][ELM6][NUM14]': 807,\n",
       " '[ELM1][NUM13][ELM6][NUM15]': 808,\n",
       " '[ELM1][NUM29][ELM6][NUM29]': 809,\n",
       " '[ELM1][NUM18][ELM6][NUM14]': 810,\n",
       " '[ELM1][NUM30][ELM6][NUM25]': 811,\n",
       " '[ELM1][NUM11][ELM6][NUM15]': 812,\n",
       " '[ELM1][NUM18][ELM6][NUM20]': 813,\n",
       " '[ELM1][NUM23][ELM6][NUM25]': 814,\n",
       " '[ELM1][NUM24][ELM6][NUM19]': 815,\n",
       " '[ELM1][NUM32][ELM6][NUM19]': 816,\n",
       " '[ELM1][NUM21][ELM6][NUM24]': 817,\n",
       " '[ELM1][NUM27][ELM6][NUM29]': 818,\n",
       " '[ELM1][NUM14][ELM6][NUM19]': 819,\n",
       " '[ELM1][NUM19][ELM6][NUM24]': 820,\n",
       " '[ELM1][NUM29][ELM6][NUM23]': 821,\n",
       " '[ELM1][NUM24][ELM6][NUM18]': 822,\n",
       " '[ELM1][NUM33][ELM6][NUM27]': 823,\n",
       " '[ELM1][NUM27][ELM6][NUM22]': 824,\n",
       " '[ELM1][NUM19][ELM6][NUM21]': 825,\n",
       " '[ELM1][NUM23][ELM6][NUM18]': 826,\n",
       " '[ELM1][NUM25][ELM6][NUM22]': 827,\n",
       " '[ELM1][NUM16][ELM6][NUM15]': 828,\n",
       " '[NUM22][ELM6][NUM23][ELM6]': 829,\n",
       " '[ELM1][NUM20][ELM6][NUM21]': 830,\n",
       " '[ELM1][NUM25][ELM6][NUM18]': 831,\n",
       " '[ELM1][NUM29][ELM6][NUM20]': 832,\n",
       " '[ELM1][NUM26][ELM6][NUM20]': 833,\n",
       " '[ELM1][NUM26][ELM6][NUM16]': 834,\n",
       " '[ELM1][NUM28][ELM6][NUM24]': 835,\n",
       " '[ELM1][NUM28][ELM6][NUM19]': 836,\n",
       " '[ELM1][NUM27][ELM6][NUM25]': 837,\n",
       " '[ELM1][NUM22][ELM6][NUM20]': 838,\n",
       " '[ELM1][NUM22][ELM6][NUM22]': 839,\n",
       " '[ELM1][NUM34][ELM6][NUM23]': 840,\n",
       " '[ELM1][NUM33][ELM6][NUM20]': 841,\n",
       " '[ELM1][NUM30][ELM6][NUM26]': 842,\n",
       " '[ELM1][NUM25][ELM6][NUM20]': 843,\n",
       " '[ELM1][NUM15][ELM6][NUM13]': 844,\n",
       " '[ELM1][NUM34][ELM6][NUM28]': 845,\n",
       " '[ELM1][NUM17][ELM6][NUM16]': 846,\n",
       " '[ELM1][NUM20][ELM6][NUM22]': 847,\n",
       " '[ELM1][NUM18][ELM6][NUM22]': 848,\n",
       " '[ELM1][NUM20][ELM6][NUM25]': 849,\n",
       " '[ELM1][NUM28][ELM6][NUM20]': 850,\n",
       " '[ELM1][NUM18][ELM6][NUM17]': 851,\n",
       " '[ELM1][NUM14][ELM6][NUM17]': 852,\n",
       " '[ELM1][NUM14][ELM6][NUM15]': 853,\n",
       " '[ELM1][NUM22][ELM6][NUM19]': 854,\n",
       " '[ELM1][NUM26][ELM6][NUM21]': 855,\n",
       " '[ELM1][NUM21][ELM6][NUM18]': 856,\n",
       " '[ELM1][NUM32][ELM6][NUM23]': 857,\n",
       " '[ELM1][NUM25][ELM6][NUM21]': 858,\n",
       " '[ELM1][NUM32][ELM6][NUM25]': 859,\n",
       " '[ELM1][NUM19][ELM6][NUM20]': 860,\n",
       " '[ELM1][NUM32][ELM6][NUM26]': 861,\n",
       " '[ELM1][NUM25][ELM6][NUM26]': 862,\n",
       " '[ELM1][NUM29][ELM6][NUM19]': 863,\n",
       " '[ELM1][NUM21][ELM6][NUM17]': 864,\n",
       " '[ELM1][NUM21][ELM6][NUM25]': 865,\n",
       " '[ELM1][NUM30][ELM6][NUM20]': 866,\n",
       " '[ELM1][NUM25][ELM6][NUM25]': 867,\n",
       " '[ELM1][NUM15][ELM6][NUM15]': 868,\n",
       " '[ELM1][NUM25][ELM6][NUM17]': 869,\n",
       " '[ELM1][NUM26][ELM6][NUM22]': 870,\n",
       " '[ELM1][NUM26][ELM6][NUM17]': 871,\n",
       " '[ELM1][NUM21][ELM6][NUM23]': 872,\n",
       " '[ELM1][NUM24][ELM6][NUM25]': 873,\n",
       " '[ELM1][NUM14][ELM6][NUM16]': 874,\n",
       " '[ELM1][NUM21][ELM6][NUM19]': 875,\n",
       " '[ELM1][NUM32][ELM6][NUM22]': 876,\n",
       " '[ELM1][NUM31][ELM6][NUM21]': 877,\n",
       " '[ELM1][NUM23][ELM6][NUM21]': 878,\n",
       " '[ELM1][NUM27][ELM6][NUM26]': 879,\n",
       " '[ELM1][NUM25][ELM6][NUM24]': 880,\n",
       " '[ELM1][NUM20][ELM6][NUM20]': 881,\n",
       " '[ELM1][NUM22][ELM6][NUM17]': 882,\n",
       " '[ELM1][NUM13][ELM6][NUM18]': 883,\n",
       " '[ELM1][NUM23][ELM6][NUM17]': 884,\n",
       " '[ELM1][NUM16][ELM6][NUM17]': 885,\n",
       " '[ELM1][NUM25][ELM6][NUM19]': 886,\n",
       " '[ELM1][NUM23][ELM6][NUM23]': 887,\n",
       " '[ELM1][NUM12][ELM6][NUM13]': 888,\n",
       " '[ELM1][NUM20][ELM6][NUM16]': 889,\n",
       " '[ELM1][NUM31][ELM6][NUM23]': 890,\n",
       " '[ELM1][NUM20][ELM6][NUM17]': 891,\n",
       " '[ELM1][NUM22][ELM6][NUM18]': 892,\n",
       " '[ELM1][NUM16][ELM6][NUM19]': 893,\n",
       " '[ELM1][NUM33][ELM6][NUM22]': 894,\n",
       " '[ELM1][NUM37][ELM6][NUM29]': 895,\n",
       " '[ELM1][NUM21][ELM6][NUM22]': 896,\n",
       " '[ELM1][NUM15][ELM6][NUM18]': 897,\n",
       " '[ELM1][NUM22][ELM6][NUM16]': 898,\n",
       " '[ELM1][NUM28][ELM6][NUM25]': 899,\n",
       " '[ELM1][NUM24][ELM6][NUM21]': 900,\n",
       " '[ELM1][NUM22][ELM6][NUM25]': 901,\n",
       " '[ELM1][NUM18][ELM6][NUM15]': 902,\n",
       " '[ELM1][NUM19][ELM6][NUM18]': 903,\n",
       " '[ELM1][NUM29][ELM6][NUM24]': 904,\n",
       " '[ELM1][NUM35][ELM6][NUM27]': 905,\n",
       " '[ELM1][NUM28][ELM6][NUM28]': 906,\n",
       " '[ELM1][NUM19][ELM6][NUM22]': 907,\n",
       " '[ELM1][NUM19][ELM6][NUM16]': 908,\n",
       " '[ELM1][NUM30][ELM6][NUM18]': 909,\n",
       " '[ELM1][NUM26][ELM6][NUM24]': 910,\n",
       " '[ELM1][NUM16][ELM6][NUM13]': 911,\n",
       " '[ELM1][NUM23][ELM6][NUM26]': 912,\n",
       " '[ELM1][NUM26][ELM6][NUM19]': 913,\n",
       " '[ELM1][NUM15][ELM6][NUM16]': 914,\n",
       " '[ELM1][NUM17][ELM6][NUM15]': 915,\n",
       " '[ELM1][NUM28][ELM6][NUM23]': 916,\n",
       " '[ELM1][NUM13][ELM6][NUM13]': 917,\n",
       " '[ELM1][NUM32][ELM6][NUM20]': 918,\n",
       " '[ELM1][NUM25][ELM6][NUM16]': 919,\n",
       " '[ELM1][NUM23][ELM6][NUM22]': 920,\n",
       " '[ELM1][NUM29][ELM6][NUM30]': 921,\n",
       " '[ELM1][NUM27][ELM6][NUM20]': 922,\n",
       " '[ELM1][NUM28][ELM6][NUM26]': 923,\n",
       " '[ELM1][NUM31][ELM6][NUM19]': 924,\n",
       " '[ELM1][NUM29][ELM6][NUM21]': 925,\n",
       " '[ELM1][NUM19][ELM6][NUM17]': 926,\n",
       " '[ELM1][NUM27][ELM6][NUM23]': 927,\n",
       " '[ELM1][NUM18][ELM6][NUM18]': 928,\n",
       " '[ELM1][NUM31][ELM6][NUM20]': 929,\n",
       " '[ELM1][NUM20][ELM6][NUM23]': 930,\n",
       " '[ELM1][NUM31][ELM6][NUM29]': 931,\n",
       " '[ELM1][NUM31][ELM6][NUM22]': 932,\n",
       " '[ELM1][NUM31][ELM6][NUM26]': 933,\n",
       " '[ELM1][NUM17][ELM6][NUM21]': 934,\n",
       " '[ELM1][NUM22][ELM6][NUM26]': 935,\n",
       " '[ELM1][NUM33][ELM6][NUM26]': 936,\n",
       " '[ELM1][NUM32][ELM6][NUM27]': 937,\n",
       " '[ELM1][NUM25][ELM6][NUM28]': 938,\n",
       " '[ELM1][NUM21][ELM6][NUM15]': 939,\n",
       " '[ELM1][NUM16][ELM6][NUM21]': 940,\n",
       " '[ELM1][NUM12][ELM6][NUM15]': 941,\n",
       " '[ELM1][NUM17][ELM6][NUM13]': 942,\n",
       " '[ELM1][NUM15][ELM6][NUM17]': 943,\n",
       " '[ELM1][NUM24][ELM6][NUM16]': 944,\n",
       " '[ELM1][NUM31][ELM6][NUM28]': 945,\n",
       " '[ELM1][NUM30][ELM6][NUM21]': 946,\n",
       " '[ELM1][NUM27][ELM6][NUM28]': 947,\n",
       " '[ELM1][NUM23][ELM6][NUM24]': 948,\n",
       " '[ELM1][NUM35][ELM6][NUM30]': 949,\n",
       " '[ELM1][NUM18][ELM6][NUM23]': 950,\n",
       " '[ELM1][NUM27][ELM6][NUM19]': 951,\n",
       " '[ELM1][NUM11][ELM6][NUM13]': 952,\n",
       " '[ELM1][NUM30][ELM6][NUM30]': 953,\n",
       " '[ELM1][NUM32][ELM6][NUM21]': 954,\n",
       " '[ELM1][NUM24][ELM6][NUM27]': 955,\n",
       " '[ELM1][NUM21][ELM6][NUM26]': 956,\n",
       " '[ELM1][NUM31][ELM6][NUM24]': 957,\n",
       " '[ELM1][NUM20][ELM6][NUM18]': 958,\n",
       " '[ELM1][NUM16][ELM6][NUM16]': 959,\n",
       " '[ELM1][NUM33][ELM6][NUM31]': 960,\n",
       " '[ELM1][NUM10][ELM6][NUM15]': 961,\n",
       " '[ELM1][NUM22][ELM6][NUM21]': 962,\n",
       " '[ELM1][NUM14][ELM6][NUM13]': 963,\n",
       " '[ELM1][NUM28][ELM6][NUM18]': 964,\n",
       " '[ELM1][NUM29][ELM6][NUM17]': 965,\n",
       " '[ELM1][NUM23][ELM6][NUM16]': 966,\n",
       " '[ELM1][NUM34][ELM6][NUM29]': 967,\n",
       " '[ELM1][NUM17][ELM6][NUM18]': 968,\n",
       " '[ELM1][NUM19][ELM6][NUM23]': 969,\n",
       " '[ELM1][NUM28][ELM6][NUM29]': 970,\n",
       " '[ELM1][NUM26][ELM6][NUM18]': 971,\n",
       " '[ELM1][NUM13][ELM6][NUM16]': 972,\n",
       " '[ELM1][NUM20][ELM6][NUM24]': 973,\n",
       " '[ELM1][NUM18][ELM6][NUM16]': 974,\n",
       " '[ELM1][NUM20][ELM6][NUM15]': 975,\n",
       " '[ELM1][NUM15][ELM6][NUM21]': 976,\n",
       " '[ELM1][NUM13][ELM6][NUM17]': 977,\n",
       " '[ELM1][NUM33][ELM6][NUM25]': 978,\n",
       " '[ELM1][NUM32][ELM6][NUM29]': 979,\n",
       " '[ELM1][NUM14][ELM6][NUM18]': 980,\n",
       " '[ELM1][NUM19][ELM6][NUM19]': 981,\n",
       " '[ELM1][NUM32][ELM6][NUM28]': 982,\n",
       " '[ELM1][NUM25][ELM6][NUM27]': 983,\n",
       " '[ELM1][NUM22][ELM6][NUM24]': 984,\n",
       " '[ELM1][NUM24][ELM6][NUM20]': 985,\n",
       " '[ELM1][NUM20][ELM6][NUM14]': 986,\n",
       " '[ELM1][NUM26][ELM6][NUM26]': 987,\n",
       " '[ELM1][NUM15][ELM6][NUM19]': 988,\n",
       " '[ELM1][NUM35][ELM6][NUM23]': 989,\n",
       " '[ELM1][NUM31][ELM6][NUM25]': 990,\n",
       " '[ELM1][NUM36][ELM6][NUM25]': 991,\n",
       " '[ELM1][NUM32][ELM6][NUM30]': 992,\n",
       " '[ELM1][NUM38][ELM6][NUM30]': 993,\n",
       " '[ELM1][NUM27][ELM6][NUM18]': 994,\n",
       " '[ELM1][NUM27][ELM6][NUM17]': 995,\n",
       " '[ELM1][NUM29][ELM6][NUM27]': 996,\n",
       " '[ELM1][NUM27][ELM6][NUM27]': 997,\n",
       " '[ELM1][NUM24][ELM6][NUM24]': 998,\n",
       " '[ELM1][NUM15][ELM6][NUM14]': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_seq',\n",
       " 'special_tokens',\n",
       " 'smiles_tokens',\n",
       " 'keys',\n",
       " 'n_token',\n",
       " 'vocab',\n",
       " 'stop_token',\n",
       " 'pad_token',\n",
       " 'clip_token',\n",
       " 'unk_token',\n",
       " 'smiles_token',\n",
       " 'suffix_token',\n",
       " 'middle_token',\n",
       " 'graph_token',\n",
       " 'formula_token',\n",
       " 'set_token',\n",
       " 'smiles_trie',\n",
       " 'special_trie',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " 'pre_tokenize',\n",
       " 'tokenize_text',\n",
       " 'batch_smiles',\n",
       " 'decode',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__new__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'elementwise_affine',\n",
       " 'eps',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'normalized_shape',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.xformer.transformer.ln_f.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens = torch.tensor(\n",
    "    [\n",
    "        tokenizer.tokenize_text(\"[SMILES]\" + s + \"[STOP]\", pad=True)\n",
    "        if s != \"*\"\n",
    "        else tokenizer.tokenize_text(\"[SMILES]C[STOP]\", pad=True)\n",
    "        for s in smiles\n",
    "    ],\n",
    "    device=\"cpu\",\n",
    "    dtype=torch.int,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 250])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.n_seq = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_seq',\n",
       " 'special_tokens',\n",
       " 'smiles_tokens',\n",
       " 'keys',\n",
       " 'n_token',\n",
       " 'vocab',\n",
       " 'stop_token',\n",
       " 'pad_token',\n",
       " 'clip_token',\n",
       " 'unk_token',\n",
       " 'smiles_token',\n",
       " 'suffix_token',\n",
       " 'middle_token',\n",
       " 'graph_token',\n",
       " 'formula_token',\n",
       " 'set_token',\n",
       " 'smiles_trie',\n",
       " 'special_trie',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " 'pre_tokenize',\n",
       " 'tokenize_text',\n",
       " 'batch_smiles',\n",
       " 'decode',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__new__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeds = encoder.encode_tokens(batch_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Embeds the tokens, and projects into the latent space.\n",
      "\u001b[0;31mFile:\u001b[0m      /mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/coati/models/encoding/clip_e2e.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "encoder.encode_tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e3gnn_smiles_clip_e2e(\n",
       "  (point_encoder): e3gnn_clip(\n",
       "    (act_fn): SiLU()\n",
       "    (embedding): Linear(in_features=28, out_features=256, bias=True)\n",
       "    (embedding_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (node_dec): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Identity()\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (gcl_0): e_gcl_sparse(\n",
       "      (instance_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): SiLU()\n",
       "        (5): Identity()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (coord_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_1): e_gcl_sparse(\n",
       "      (instance_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): SiLU()\n",
       "        (5): Identity()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (coord_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_2): e_gcl_sparse(\n",
       "      (instance_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): SiLU()\n",
       "        (5): Identity()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (coord_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_3): e_gcl_sparse(\n",
       "      (instance_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): SiLU()\n",
       "        (5): Identity()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (coord_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (gcl_4): e_gcl_sparse(\n",
       "      (instance_norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (edge_mlp): Sequential(\n",
       "        (0): Linear(in_features=513, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (4): SiLU()\n",
       "        (5): Identity()\n",
       "      )\n",
       "      (node_mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Identity()\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (coord_mlp): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=1, bias=False)\n",
       "      )\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (xformer): RotarySmilesTransformer(\n",
       "    (norm_embed): Identity()\n",
       "    (emb): RotaryEmbedding(\n",
       "      (tok_emb): Embedding(10322, 256)\n",
       "    )\n",
       "    (transformer): ModuleDict(\n",
       "      (h): ModuleList(\n",
       "        (0-15): 16 x RotaryBlock(\n",
       "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): RotarySelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlpf): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): NewGELU()\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=256, out_features=10322, bias=False)\n",
       "  )\n",
       "  (point_to_clip): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (smiles_to_clip): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (point_clip_to_special_tokens): Sequential(\n",
       "    (0): SiLU()\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (clip_loss): clip_loss()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5.1528,  3.8842, 41.5446,  4.8210,  7.4189,  2.2844,  2.4983,  0.7494,\n",
       "         21.5638,  5.3194], grad_fn=<SumBackward1>),\n",
       " tensor([True, True, True, True, True, True, True, True, True, True]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.batch_smiles_to_s2s_likelihood(smiles, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [-11.4987,  -0.3192,  -1.5464,  ...,  -6.1698, -10.2218,  -3.1609],\n",
       "         [ -8.4435,   0.8021,  -0.8089,  ...,  -2.4167,  -7.5098,  -2.4759],\n",
       "         ...,\n",
       "         [-10.3736,   2.5105,  -0.7439,  ...,  -4.9863,  -8.6617,  -3.3883],\n",
       "         [-10.2333,   2.5897,  -0.6997,  ...,  -4.8988,  -8.5454,  -3.3025],\n",
       "         [-10.0865,   2.5194,  -0.6686,  ...,  -4.8681,  -8.4047,  -3.2709]],\n",
       "\n",
       "        [[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [-11.6163,  -2.5803,  -1.9344,  ...,  -5.1848, -10.3400,  -5.3778],\n",
       "         [ -9.6973,   3.3149,  -1.9919,  ...,  -2.9876,  -7.5202,  -5.2770],\n",
       "         ...,\n",
       "         [-10.5352,   4.5065,  -0.2985,  ...,  -4.9456,  -8.7423,  -3.3941],\n",
       "         [-10.7862,   4.4251,  -0.2186,  ...,  -5.1446,  -8.9914,  -3.7423],\n",
       "         [-10.8848,   4.3139,  -0.1930,  ...,  -5.3303,  -9.0748,  -3.8258]],\n",
       "\n",
       "        [[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [-11.2945,  -1.3544,  -2.2660,  ...,  -5.4751,  -9.9416,  -2.5143],\n",
       "         [ -8.5155,  -0.1764,  -1.4301,  ...,  -2.0406,  -7.0007,  -5.2418],\n",
       "         ...,\n",
       "         [-10.4769,   1.7416,  -0.3790,  ...,  -5.8606,  -8.8854,  -3.5118],\n",
       "         [-10.6162,   1.5881,  -0.4304,  ...,  -6.0501,  -9.0231,  -3.5678],\n",
       "         [-10.5246,   1.6435,  -0.4174,  ...,  -6.2020,  -8.9439,  -3.5919]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [-10.4360,  -0.8232,  -1.6206,  ...,  -3.6111,  -8.8612,  -3.6012],\n",
       "         [-11.8625,  -1.8163,  -2.1352,  ...,  -4.0039, -10.4337,  -4.1952],\n",
       "         ...,\n",
       "         [-11.6257,   2.8077,  -0.2967,  ...,  -6.1792, -10.0194,  -4.0602],\n",
       "         [-11.6594,   2.7186,  -0.2889,  ...,  -6.1942, -10.0298,  -4.1958],\n",
       "         [-11.7370,   2.6455,  -0.3014,  ...,  -6.2925, -10.0717,  -4.3035]],\n",
       "\n",
       "        [[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [-11.9132,   0.4160,  -0.3498,  ...,  -8.4536, -11.0736,  -6.6369],\n",
       "         [ -9.5690,   2.2645,  -2.6393,  ...,  -3.9659,  -8.7057,  -3.6713],\n",
       "         ...,\n",
       "         [ -8.5619,   5.5227,  -0.7811,  ...,  -3.9811,  -6.6036,  -2.8310],\n",
       "         [ -8.6347,   5.6740,  -0.7166,  ...,  -4.0606,  -6.7004,  -2.8271],\n",
       "         [ -8.5900,   5.7397,  -0.7063,  ...,  -4.0806,  -6.6832,  -2.7069]],\n",
       "\n",
       "        [[ -4.7370,   0.3078,   1.6261,  ...,  -0.0516,  -3.1638,   0.5547],\n",
       "         [ -9.4403,  -0.3705,  -0.9194,  ...,  -3.9224,  -8.0706,  -4.7252],\n",
       "         [ -6.9940,   1.4206,  -1.3331,  ...,  -2.0259,  -5.6786,  -2.0533],\n",
       "         ...,\n",
       "         [-10.8106,   3.6643,  -0.1279,  ...,  -6.1839,  -9.0310,  -3.8985],\n",
       "         [-10.9586,   3.6245,  -0.1119,  ...,  -6.3850,  -9.1505,  -4.1002],\n",
       "         [-10.9976,   3.5159,  -0.1578,  ...,  -6.5804,  -9.1571,  -4.1595]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.xformer.forward(batch_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(version_base=None, config_path=\"../configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(\n",
    "    config_name=\"train.yaml\",\n",
    "    overrides=[\n",
    "        \"evaluate=true\",\n",
    "        \"eval=hint\",\n",
    "        \"paths.projects_dir=..\",\n",
    "        \"paths.output_dir=./tmp/21312FS12A\",\n",
    "        \"trainer.devices=1\",\n",
    "        \"seed=22123\",\n",
    "        \"experiment=coati/med\",\n",
    "        \"trainer=gpu\",\n",
    "        \"trainer.devices=[1]\",\n",
    "        \"trainer.max_epochs=200\",\n",
    "        \"data.num_workers=12\",\n",
    "        \"data.transform.size=224\",\n",
    "        \"data.batch_size=4\",\n",
    "        \"model.embedding_dim=256\",\n",
    "        \"model/image_encoder=vit_base_16_224\",\n",
    "        \"model/criterion=ntxent_reg\",\n",
    "        \"model.criterion.alpha=0.2\",\n",
    "        \"model.criterion.mse_reg=0.5\",\n",
    "        \"model.criterion.variance_reg=1\",\n",
    "        \"model.criterion.covariance_reg=0.25\",\n",
    "        \"model.criterion.temperature=10\",\n",
    "        \"model.criterion.temperature_requires_grad=True\",\n",
    "    ],\n",
    ")\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "dm = instantiate(cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.get(\"load_first_bacth\"):\n",
    "    dm.prepare_data()\n",
    "    dm.setup(\"fit\")\n",
    "    dl = dm.train_dataloader(batch_size=2)\n",
    "    b = next(iter(dl))\n",
    "    example_input = b\n",
    "else:\n",
    "    example_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 17.92M Total: 20.36M \n"
     ]
    }
   ],
   "source": [
    "model = instantiate(cfg.model, example_input=example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.trainer.devices = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:397: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 17.92M Total: 20.36M \n"
     ]
    }
   ],
   "source": [
    "model: LightningModule = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n",
    "\n",
    "logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n",
    "\n",
    "trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.example_input_array = example_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_emb': tensor([[ 1.6610e-02, -4.1819e-01, -4.4254e-02,  1.2422e+00,  6.1211e-01,\n",
       "          -2.6903e-01, -2.6012e-01,  4.2256e-01, -1.7027e-01,  3.6687e-01,\n",
       "           8.9104e-01,  3.1795e-01,  2.8448e-01,  4.2938e-01, -2.4188e-01,\n",
       "          -5.4162e-01,  4.1095e-01,  3.4153e-01, -4.0051e-01,  3.8637e-01,\n",
       "          -3.0858e-01, -7.8681e-02, -1.7590e-01,  4.3911e-01, -5.6159e-01,\n",
       "          -9.1884e-01,  7.0309e-02, -4.9634e-01,  7.9358e-01,  1.6343e-01,\n",
       "          -8.4752e-01,  9.3032e-01, -3.9207e-01,  2.7210e-01, -5.9688e-02,\n",
       "           9.2509e-02, -5.6339e-01,  4.5488e-01, -5.6979e-01, -8.1515e-02,\n",
       "           4.2378e-01,  1.8099e-01, -3.3752e-01, -6.2349e-01, -8.4275e-02,\n",
       "           3.3699e-02,  1.6890e-01, -4.9607e-02,  6.8864e-01, -3.5666e-01,\n",
       "           4.0561e-01, -3.0659e-01,  2.1460e-01, -9.7925e-01,  3.7538e-03,\n",
       "          -9.8630e-01,  2.2101e-01, -1.4444e-01, -2.0538e-01,  2.4341e-01,\n",
       "           4.0621e-01, -6.1124e-01,  9.6070e-01, -7.7939e-01,  5.6610e-01,\n",
       "           6.0083e-01,  5.2502e-01, -3.3839e-02,  4.9607e-01,  4.6455e-01,\n",
       "           2.7790e-01, -3.2230e-01,  8.3178e-02, -2.3351e-02, -1.2416e-01,\n",
       "           3.7916e-01,  5.8555e-01,  3.3756e-01, -7.9289e-01,  3.1989e-02,\n",
       "           2.1156e-02,  8.7485e-01,  1.3058e-01,  2.4081e-01,  1.1571e-01,\n",
       "           3.2501e-01,  3.2905e-01,  2.5027e-01, -6.1960e-01, -8.9110e-02,\n",
       "           5.1970e-02, -1.1558e+00, -7.8430e-02,  6.7234e-02,  3.9722e-01,\n",
       "           8.5179e-02, -5.9347e-01,  5.5791e-01,  2.7632e-01, -3.1848e-02,\n",
       "          -1.5285e-02, -1.2341e-01, -7.2341e-01,  1.0549e-01,  8.7803e-02,\n",
       "           7.5008e-01, -3.9476e-01,  1.4539e-01,  3.7600e-01,  1.0987e+00,\n",
       "          -5.9536e-01,  2.3536e-01, -2.7851e-02, -2.8185e-01,  5.0710e-01,\n",
       "           1.0762e-01, -4.6886e-01, -2.3270e-01, -2.3889e-01, -2.2465e-01,\n",
       "           2.9700e-01, -3.7120e-01,  1.3220e-01, -3.8093e-02, -6.0695e-01,\n",
       "           1.1991e-01,  8.6007e-01,  6.2281e-02,  1.3205e-01,  7.3570e-02,\n",
       "           3.4492e-01,  8.4601e-02, -7.6482e-01, -4.6039e-01,  2.1557e-01,\n",
       "           9.0626e-02,  2.0555e-01,  1.5872e-01,  7.9999e-02,  5.7958e-01,\n",
       "           6.3580e-01,  3.0209e-01,  1.3941e-01,  1.6276e-01,  1.9523e-01,\n",
       "          -6.2831e-04, -5.1938e-01,  3.1600e-01,  1.4920e-01,  1.8099e-01,\n",
       "           6.5908e-01, -7.6239e-02, -3.9322e-01,  9.8797e-02, -1.4043e+00,\n",
       "          -2.9028e-01,  3.6371e-01, -2.0382e-01,  3.6389e-02,  2.3494e-01,\n",
       "          -1.4306e-01, -7.3209e-01, -3.7051e-01,  8.2574e-01,  3.0303e-01,\n",
       "           2.0805e-01, -1.0504e-01,  4.5574e-01, -2.6387e-01,  8.2759e-02,\n",
       "           3.4199e-01,  6.7194e-01, -2.6841e-01,  3.3780e-03,  1.4109e-01,\n",
       "          -2.2342e-01, -4.9790e-01, -2.3434e-01, -2.0751e-02,  4.8844e-01,\n",
       "          -6.0029e-01,  4.3626e-01, -6.7282e-01,  1.5677e-01, -5.8093e-01,\n",
       "          -1.2398e-01,  5.2213e-01,  8.2454e-02, -6.8218e-02,  1.9466e-02,\n",
       "          -9.6190e-02,  3.2200e-01, -2.4281e-01, -6.7762e-01,  1.4908e-01,\n",
       "          -7.1459e-01, -7.9423e-01,  3.9945e-01,  4.7208e-01, -5.5665e-01,\n",
       "          -7.6005e-02,  4.0259e-01, -4.9001e-01,  1.0707e-01,  8.0931e-01,\n",
       "           8.3112e-03,  7.0923e-01, -2.3001e-01,  1.3923e-01,  6.1117e-01,\n",
       "          -3.8512e-01, -2.5512e-01, -3.0434e-01,  2.9766e-01, -5.4987e-01,\n",
       "          -1.4379e-01, -1.6595e-01,  1.0201e+00,  4.3471e-01, -5.0152e-01,\n",
       "          -6.1377e-01,  3.9279e-01, -1.6987e-02,  1.1565e+00,  2.8524e-01,\n",
       "           6.2305e-01, -4.2485e-01, -1.9376e-01, -2.7650e-01,  2.9382e-01,\n",
       "           4.4764e-01,  5.4419e-01, -6.3820e-01,  2.2213e-01, -1.0529e-01,\n",
       "          -3.2988e-01,  1.3254e-01, -4.0067e-01,  6.2316e-01, -8.9496e-01,\n",
       "           3.8642e-01, -2.9704e-01,  2.8308e-01, -2.3993e-01, -6.5483e-03,\n",
       "          -1.6941e-02, -3.7991e-01,  7.7887e-01,  5.8578e-01,  7.2135e-01,\n",
       "          -2.3348e-01,  1.0872e-01, -5.9478e-02, -4.8703e-01, -9.2983e-01,\n",
       "          -1.2178e+00],\n",
       "         [ 8.4253e-02, -1.8094e-02,  4.1051e-01,  6.3818e-01,  6.2912e-01,\n",
       "          -3.8298e-02, -1.3281e-01, -4.3846e-02, -5.0165e-01,  3.6633e-01,\n",
       "           1.1315e+00, -2.4251e-02,  3.0520e-01,  5.0867e-01,  1.9020e-01,\n",
       "          -4.7015e-01,  4.6173e-01,  4.0623e-02,  2.6752e-01,  1.4506e-01,\n",
       "          -4.6086e-01,  5.3714e-01,  1.0496e-01,  4.3167e-03,  6.9320e-02,\n",
       "          -4.9199e-01,  3.2484e-01, -8.0966e-01,  6.1796e-01, -5.1590e-01,\n",
       "          -2.9255e-01,  4.7111e-01, -3.6425e-01, -4.3177e-01, -2.4259e-02,\n",
       "           1.7784e-01, -7.2026e-02,  3.7002e-01, -6.4421e-01, -6.6407e-02,\n",
       "           2.4866e-01, -1.9452e-01,  8.0641e-03, -1.6260e-01, -9.2909e-02,\n",
       "           1.2147e-01,  3.6339e-01, -3.3948e-01,  8.2866e-01, -2.3422e-01,\n",
       "           6.1333e-01,  7.2266e-02,  1.4028e-01, -4.5718e-01,  9.0781e-02,\n",
       "          -9.7476e-01, -1.1021e-01,  4.7775e-01, -3.6199e-01,  2.6433e-01,\n",
       "           2.3315e-01, -2.5232e-01,  5.5266e-02, -7.3486e-01, -1.0745e-01,\n",
       "           1.7762e-01,  2.9244e-01,  6.2730e-01,  3.3985e-01,  1.5064e-01,\n",
       "           6.9777e-02, -2.7189e-01,  9.1597e-02, -3.5434e-01, -5.8661e-01,\n",
       "           5.3712e-01,  3.3159e-01,  3.4466e-01, -7.2929e-01, -1.9355e-01,\n",
       "          -3.7893e-02,  6.6609e-01, -8.9721e-02,  9.6085e-02,  1.6937e-01,\n",
       "           3.6947e-01,  6.5119e-01,  5.0216e-02, -2.0465e-02,  3.5340e-01,\n",
       "          -3.0277e-01, -5.7295e-01, -3.2232e-01, -1.6803e-01, -1.5292e-01,\n",
       "          -4.6181e-01, -3.5018e-01, -2.8826e-02,  5.1180e-01,  3.8565e-01,\n",
       "           2.0644e-02, -1.8980e-02,  1.6446e-01, -1.4082e-01,  1.2899e-01,\n",
       "           4.2484e-03, -3.5002e-01, -2.0406e-02,  3.6247e-01,  4.7464e-01,\n",
       "           5.3381e-01,  3.0748e-01,  5.6943e-02, -2.1975e-01,  3.1332e-01,\n",
       "           1.3627e-01, -6.2105e-02,  1.1335e-01, -7.7060e-01,  1.0752e-01,\n",
       "           6.7935e-01,  6.0626e-01,  5.3737e-02,  4.2136e-03, -4.9955e-01,\n",
       "          -2.8978e-01,  6.6492e-01, -1.2999e-01,  3.3711e-02, -5.6340e-02,\n",
       "           3.8878e-01,  3.9636e-01, -4.4944e-01, -4.2083e-01,  2.4523e-02,\n",
       "          -9.1486e-02,  2.3027e-01,  8.9241e-02,  3.3826e-01,  3.4315e-01,\n",
       "          -1.8449e-01,  6.7095e-01, -1.3302e-01, -5.4135e-02,  3.1394e-01,\n",
       "          -7.2562e-01, -6.6140e-01,  1.9387e-01, -4.9644e-01, -2.0458e-01,\n",
       "           3.0458e-01, -2.9772e-02,  8.6504e-02,  9.8991e-02, -3.3192e-01,\n",
       "          -3.7989e-01,  5.4927e-02, -1.4200e-01,  4.1801e-01,  1.1266e-01,\n",
       "          -1.1549e-01, -7.8195e-01, -2.5092e-01,  5.1745e-01,  9.9218e-02,\n",
       "           4.8569e-01,  3.1961e-02,  3.6730e-01, -8.8854e-02,  1.0771e-01,\n",
       "          -1.2783e-01,  5.4604e-01, -3.4559e-01,  1.1741e-01, -1.9934e-01,\n",
       "           1.1205e-01, -4.5835e-01,  2.4152e-01, -3.8383e-01,  1.4798e-01,\n",
       "          -5.9798e-01,  4.7188e-02, -3.5087e-01,  1.9162e-01, -6.2040e-01,\n",
       "          -3.0521e-01,  1.5420e-01, -1.2617e-01, -1.3482e-01, -1.5551e-01,\n",
       "           2.7320e-01,  2.8074e-04,  1.8229e-01, -2.3896e-01, -6.8349e-03,\n",
       "          -6.7553e-01,  2.1118e-01, -2.7081e-01,  3.3116e-01, -4.0898e-01,\n",
       "          -2.4857e-01,  1.1255e-01, -5.1107e-02,  4.1109e-01,  1.9974e-01,\n",
       "          -8.0697e-02,  2.8282e-01, -2.8619e-01, -3.4739e-01,  4.8223e-01,\n",
       "          -3.9928e-01, -1.2554e-01, -3.1126e-01, -1.3248e-01, -8.1503e-01,\n",
       "          -5.4329e-01, -1.3933e-01,  6.5122e-01,  1.9087e-01, -6.4527e-02,\n",
       "          -3.7496e-02,  8.9326e-01,  2.4646e-02,  4.1488e-01,  1.3626e-01,\n",
       "          -9.5393e-02, -2.8133e-01, -1.2934e-02,  1.9415e-01,  1.1409e-01,\n",
       "           3.6573e-01,  4.1090e-01, -5.6412e-01, -2.5755e-01,  3.5865e-01,\n",
       "          -2.5292e-02, -1.4667e-01, -2.9839e-01,  4.3777e-01, -3.7097e-01,\n",
       "           2.7804e-01, -5.1620e-01,  7.9005e-02, -2.3172e-01, -3.1161e-01,\n",
       "          -5.5106e-01, -1.8076e-01,  5.5267e-01,  1.1932e+00,  1.2830e-01,\n",
       "          -1.4789e-01,  5.4770e-03,  3.6149e-01, -2.2199e-01, -3.4388e-01,\n",
       "          -6.6920e-01]], grad_fn=<AddmmBackward0>),\n",
       " 'compound_emb': tensor([[-0.2202,  0.4899, -0.1601, -0.0014, -0.7943, -0.4439, -0.7137, -0.2114,\n",
       "           0.4261, -0.5817,  0.2946,  0.0344,  0.7052, -0.3732, -0.7302, -0.1360,\n",
       "           0.3435,  0.6430,  0.0995, -0.1778, -0.0806, -0.3535, -0.3653, -0.2839,\n",
       "           0.2893,  0.1142, -0.1256, -0.5486,  0.5793,  0.2779, -0.0927,  0.2358,\n",
       "          -0.0307,  0.0734,  0.3881, -0.2689,  0.1735,  0.0859, -0.3024,  0.4088,\n",
       "          -0.0480, -0.5010, -0.3564, -0.7579, -0.7143,  0.0040, -0.1698, -0.9307,\n",
       "           0.5320, -0.2544, -0.0546, -0.3100, -0.5060, -0.1497, -0.1588, -0.5822,\n",
       "          -0.7252,  0.7649,  0.1234,  0.7220, -0.1961, -0.0229, -0.2062,  0.0257,\n",
       "           0.1750, -0.1647,  0.4431,  0.0190, -0.8161,  0.2026,  0.2585,  0.0301,\n",
       "           0.7965, -0.3568, -0.4621, -0.4479,  0.0298, -0.5554,  0.5072, -0.6102,\n",
       "           0.0179, -0.4700, -0.2064,  0.8480, -0.2219,  0.3842,  0.1052,  0.4460,\n",
       "          -0.0380,  0.0137,  0.2716,  0.4459, -0.2764,  0.3542, -0.2391, -0.8818,\n",
       "           0.9208,  0.1224,  0.1468, -0.0554, -0.0496,  0.2906,  0.1611, -0.0979,\n",
       "           0.5756, -0.1704, -0.1176, -0.5641, -0.3014, -0.2557, -0.4232, -0.1077,\n",
       "          -0.0793, -0.3316,  0.1320, -0.0778,  0.3858,  0.4827, -0.3591, -0.1373,\n",
       "          -0.0615, -0.1747, -0.1939, -0.3018,  0.2719, -0.1323, -0.6913,  0.2708,\n",
       "           0.4432, -0.6266,  0.6301,  0.8173,  0.3535, -0.5277, -0.1845,  0.0092,\n",
       "          -0.0410,  0.5765,  0.1069, -0.0402, -0.0910, -0.5077,  0.4988,  0.6356,\n",
       "           0.1451, -0.4407, -0.4408, -0.3623, -0.4137, -0.2747, -0.6499, -0.0837,\n",
       "          -0.5897, -0.1416,  1.1307, -0.0707,  0.0069,  0.0927,  0.3150, -0.5265,\n",
       "           0.0546, -0.2227, -0.3247, -0.2492,  0.0816, -0.5468,  0.3162, -0.0286,\n",
       "          -0.2242, -0.4011, -0.4675, -0.3684, -0.1026,  0.0139, -0.2559,  0.3096,\n",
       "          -0.3950, -0.0583, -0.2616,  0.1208,  0.2419, -0.2720,  0.6259, -0.5000,\n",
       "           0.0730,  0.6639,  0.3102,  1.1697,  0.3583, -0.3532,  0.3452,  0.0064,\n",
       "          -0.0776,  0.3080,  0.7737, -0.1596, -0.7971, -0.3981, -0.2893, -0.4442,\n",
       "          -0.2961,  0.6184, -0.6146,  0.3227, -0.0376, -0.2441, -0.8947, -0.9494,\n",
       "          -0.2364,  0.4580, -0.0548, -0.1191, -0.1298,  0.3283, -0.4709,  0.0023,\n",
       "           0.3539,  0.1709,  0.0920,  0.4094,  0.3502, -0.4710, -0.6486, -0.5252,\n",
       "          -0.0766, -0.8047,  0.4476,  0.4232,  0.8055, -0.5693,  0.1794, -0.3802,\n",
       "           0.7443,  0.2505, -0.3155, -0.6802, -0.2531,  0.3007, -0.7746, -0.2903,\n",
       "           0.4785,  0.3181, -0.4062,  0.0427,  0.0193, -0.3745,  0.4640,  0.2980,\n",
       "          -0.8770,  0.4495,  0.6448,  0.0356,  0.0983,  0.2037,  0.7005, -0.2299],\n",
       "         [ 0.0682,  0.1211,  0.3816, -0.4930, -0.7570, -0.0189, -0.3545, -0.4371,\n",
       "           0.9132, -0.2078, -0.3197, -0.6156,  0.4130, -0.3552, -0.2250,  0.1858,\n",
       "           0.4356,  0.3938,  0.5825, -0.1090,  0.3182, -0.1572,  0.4649,  0.4606,\n",
       "           0.7575,  0.5305,  0.0319, -0.3029, -0.1871,  0.0844, -0.7935,  0.1471,\n",
       "           0.4503, -0.8947,  0.5116, -0.1028,  0.3223,  0.0311, -0.0066, -0.0608,\n",
       "          -0.2446, -0.4280,  0.4659,  0.2094, -0.6372,  0.2322, -0.4777, -0.2739,\n",
       "           1.0146, -0.3174, -0.1034,  0.1782,  0.0646,  0.4869,  0.0227,  0.2469,\n",
       "          -0.1900,  0.7127, -0.2143,  0.5779, -0.3383, -0.3829, -0.1605, -0.3016,\n",
       "          -0.2385, -0.6138,  0.0349,  0.0714, -0.2297, -0.4250, -0.3700, -0.4683,\n",
       "           0.0521,  0.0394, -0.3230, -0.7723,  0.1817,  0.2628,  0.0168, -0.0013,\n",
       "          -0.5817, -0.9291,  1.0165,  0.4687, -0.2046,  0.5046, -0.0410, -0.2525,\n",
       "           0.2508, -0.6487,  0.4105,  0.0498, -0.2568,  0.2960,  0.5863, -0.0688,\n",
       "          -0.2166,  0.6405,  0.1066, -0.2582,  0.6431, -0.2860,  0.9355, -0.0804,\n",
       "           0.0131, -0.1942, -0.4070, -0.0276, -0.1351, -0.4024, -0.0721,  0.2028,\n",
       "          -0.4997, -0.0270, -0.0449, -0.2180, -0.2972,  0.3859,  0.7178, -0.3247,\n",
       "           0.3303, -0.1776,  0.4243,  0.2482,  0.5300, -0.0403, -0.1991,  0.3547,\n",
       "           0.3837, -0.4002,  0.1049,  0.4320,  0.4236,  0.3372,  0.0926, -0.3583,\n",
       "          -0.0091, -0.2477,  0.0363, -0.1502,  0.9187,  0.1399, -0.3392,  0.0724,\n",
       "          -0.4379,  0.6344, -0.2471, -0.1169,  0.0957, -0.0961,  0.5037, -0.3152,\n",
       "           0.8742,  0.1229,  0.0195, -0.2172, -0.1534, -0.0930,  0.3717, -0.3640,\n",
       "           0.1519, -1.1844, -0.4724, -0.4146,  0.5557, -0.1106,  0.1346, -0.2414,\n",
       "          -0.4663,  0.3394, -0.6166, -0.1469, -0.6634,  0.1037,  0.0153, -0.0697,\n",
       "           0.9963,  0.1380, -0.1868,  0.8600,  0.1533,  0.3112,  0.6111, -0.6233,\n",
       "          -0.1773,  0.0050,  0.3674,  0.4943,  0.5808,  0.2026, -0.1364, -0.0570,\n",
       "           0.7642, -0.2963,  0.5437,  0.5223, -0.1575, -0.3021,  0.5970,  0.2456,\n",
       "           0.1698,  0.2095,  0.8169,  0.0816,  0.0381, -0.1585, -0.6031, -0.6380,\n",
       "          -0.5434, -0.2407, -0.1608, -0.4944,  0.4587, -0.0040, -0.1176,  0.9066,\n",
       "           0.3299, -0.1714, -0.4172,  0.0142,  0.2603,  0.1176, -0.7174,  0.7026,\n",
       "          -0.0117, -0.3312,  0.0503,  0.5882,  0.4275, -0.3564,  0.2052, -0.1569,\n",
       "          -0.0920,  0.0586, -0.2995,  0.3418,  0.2114, -0.2494, -1.2115,  0.1061,\n",
       "          -0.3106, -0.0549,  0.6159, -0.2414,  0.3936, -0.5997,  0.6740,  0.0752,\n",
       "          -0.3224, -0.3156, -0.0936, -0.1839,  0.5964, -0.0141,  0.3348,  0.0396]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**model.example_input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">         In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Out sizes </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ image_encoder                    │ CNNEncoder              │ 86.5 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [2, 5, 224, 224] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [2, 256] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ image_encoder.backbone           │ VisionTransformer       │ 86.2 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [2, 5, 224, 224] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [2, 768] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ image_encoder.projection_head    │ Sequential              │  262 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         [2, 768] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [2, 256] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ image_encoder.dropouts           │ ModuleList              │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ molecule_encoder                 │ COATI                   │ 18.1 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         [2, 250] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [2, 256] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ molecule_encoder.backbone        │ RotarySmilesTransformer │ 17.9 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ molecule_encoder.projection_head │ Sequential              │  132 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         [2, 256] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [2, 256] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ criterion                        │ RegNTXent               │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ train_loss                       │ MeanMetric              │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ val_loss                         │ MeanMetric              │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ test_loss                        │ MeanMetric              │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">         ? </span>│\n",
       "└────┴──────────────────────────────────┴─────────────────────────┴────────┴──────────────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m        In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mOut sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ image_encoder                    │ CNNEncoder              │ 86.5 M │\u001b[37m \u001b[0m\u001b[37m[2, 5, 224, 224]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m [2, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ image_encoder.backbone           │ VisionTransformer       │ 86.2 M │\u001b[37m \u001b[0m\u001b[37m[2, 5, 224, 224]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m [2, 768]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ image_encoder.projection_head    │ Sequential              │  262 K │\u001b[37m \u001b[0m\u001b[37m        [2, 768]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m [2, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ image_encoder.dropouts           │ ModuleList              │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ molecule_encoder                 │ COATI                   │ 18.1 M │\u001b[37m \u001b[0m\u001b[37m        [2, 250]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m [2, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ molecule_encoder.backbone        │ RotarySmilesTransformer │ 17.9 M │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ molecule_encoder.projection_head │ Sequential              │  132 K │\u001b[37m \u001b[0m\u001b[37m        [2, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m [2, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ criterion                        │ RegNTXent               │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ train_loss                       │ MeanMetric              │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ val_loss                         │ MeanMetric              │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ test_loss                        │ MeanMetric              │      0 │\u001b[37m \u001b[0m\u001b[37m               ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "└────┴──────────────────────────────────┴─────────────────────────┴────────┴──────────────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 104 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 104 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 418                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 104 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 104 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 418                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c63835f34a1496abc01d7573d04cc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:53: \n",
       "UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:53: \n",
       "UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jump_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
