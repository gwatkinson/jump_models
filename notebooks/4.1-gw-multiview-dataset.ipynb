{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developping the multiview dataset, datamodule, model and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "\n",
    "from src.coati.models.io import load_e3gnn_smiles_clip_e2e\n",
    "from src.modules.collate_fn import default_collate\n",
    "from src.modules.molecules.coati import COATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting cpjump1...\n",
      "Mounting cpjump2...\n",
      "Mounting cpjump3...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    if not Path(f\"../cpjump{i}/jump/\").exists():\n",
    "        print(f\"Mounting cpjump{i}...\")\n",
    "        os.system(f\"sshfs bioclust:/projects/cpjump{i}/ ../cpjump{i}\")\n",
    "    else:\n",
    "        print(f\"cpjump{i} already mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initialize(version_base=None, config_path=\"../configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_name: train\n",
      "tags:\n",
      "- small_jump_cl\n",
      "- nlp_coati\n",
      "- clip_like\n",
      "- ${model.molecule_encoder.pretrained_name}\n",
      "- ${model.image_encoder.instance_model_name}\n",
      "train: true\n",
      "test: true\n",
      "evaluate: true\n",
      "compile: false\n",
      "ckpt_path: null\n",
      "seed: 12345\n",
      "data:\n",
      "  compound_transform:\n",
      "    _target_: src.modules.compound_transforms.coati.COATITransform\n",
      "    compound_str_type: inchi\n",
      "  _target_: src.models.jump_cl.datamodule.BasicJUMPDataModule\n",
      "  batch_size: 32\n",
      "  num_workers: 24\n",
      "  pin_memory: null\n",
      "  prefetch_factor: 3\n",
      "  drop_last: true\n",
      "  transform:\n",
      "    _target_: src.modules.transforms.DefaultJUMPTransform\n",
      "    _convert_: object\n",
      "    size: 128\n",
      "    dim:\n",
      "    - -2\n",
      "    - -1\n",
      "  force_split: false\n",
      "  splitter:\n",
      "    _target_: src.splitters.ScaffoldSplitter\n",
      "    train: 1024\n",
      "    test: 256\n",
      "    val: 128\n",
      "    retrieval: 0\n",
      "  use_compond_cache: false\n",
      "  data_root_dir: ${paths.projects_dir}/\n",
      "  split_path: ${paths.split_path}/fp_small3/\n",
      "  dataloader_config:\n",
      "    train:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: ${data.drop_last}\n",
      "      shuffle: true\n",
      "    val:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "    test:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "  image_metadata_path: ${paths.metadata_path}/images_metadata.parquet\n",
      "  compound_metadata_path: ${paths.metadata_path}/compound_dict.json\n",
      "  compound_col: Metadata_InChI\n",
      "  image_sampler: null\n",
      "  metadata_dir: ${paths.raw_metadata_path}/complete_metadata.csv\n",
      "  local_load_data_dir: ${paths.load_data_path}/final/\n",
      "  index_str: '{Metadata_Source}__{Metadata_Batch}__{Metadata_Plate}__{Metadata_Well}__{Metadata_Site}'\n",
      "  channels:\n",
      "  - DNA\n",
      "  - AGP\n",
      "  - ER\n",
      "  - Mito\n",
      "  - RNA\n",
      "  col_fstring: FileName_Orig{channel}\n",
      "  id_cols:\n",
      "  - Metadata_Source\n",
      "  - Metadata_Batch\n",
      "  - Metadata_Plate\n",
      "  - Metadata_Well\n",
      "  extra_cols:\n",
      "  - Metadata_PlateType\n",
      "  - Metadata_Site\n",
      "model:\n",
      "  image_encoder:\n",
      "    _target_: src.modules.images.timm_pretrained.CNNEncoder\n",
      "    instance_model_name: resnet18\n",
      "    target_num: ${model.embedding_dim}\n",
      "    n_channels: 5\n",
      "    pretrained: true\n",
      "  molecule_encoder:\n",
      "    _target_: src.modules.molecules.coati.COATI\n",
      "    pretrained_name: grande_closed\n",
      "    out_dim: ${model.embedding_dim}\n",
      "    padding_length: 250\n",
      "    freeze: false\n",
      "  criterion:\n",
      "    _target_: src.modules.losses.contrastive_losses.InfoNCE\n",
      "    norm: true\n",
      "    temperature: 1\n",
      "    return_rank: true\n",
      "    temperature_requires_grad: false\n",
      "    temperature_min: 0\n",
      "    temperature_max: 100\n",
      "  optimizer:\n",
      "    _target_: torch.optim.Adam\n",
      "    _partial_: true\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "    eps: 1.0e-08\n",
      "    weight_decay: 0.001\n",
      "    amsgrad: false\n",
      "    lr: ${model.lr}\n",
      "  scheduler:\n",
      "    _target_: src.modules.lr_schedulers.warmup_wrapper.WarmUpWrapper\n",
      "    _partial_: true\n",
      "    warmup_steps:\n",
      "    - 5\n",
      "    interpolation: linear\n",
      "    wrapped_scheduler: ReduceLROnPlateau\n",
      "    cooldown: 3\n",
      "    factor: 0.6\n",
      "    patience: 7\n",
      "    min_lr: 1.0e-06\n",
      "    threshold: 0.0001\n",
      "    mode: min\n",
      "    verbose: true\n",
      "  _target_: src.models.jump_cl.module.BasicJUMPModule\n",
      "  embedding_dim: 256\n",
      "  lr: 0.0003\n",
      "  batch_size: ${data.batch_size}\n",
      "  example_input_path: null\n",
      "  monitor: val/loss\n",
      "  interval: epoch\n",
      "  frequency: 1\n",
      "  image_backbone: backbone\n",
      "  image_head: projection_head\n",
      "  molecule_backbone: backbone\n",
      "  molecule_head: projection_head\n",
      "callbacks:\n",
      "  rich_progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "  model_summary:\n",
      "    _target_: lightning.pytorch.callbacks.RichModelSummary\n",
      "    max_depth: 2\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: epoch_{epoch:03d}\n",
      "    monitor: val/loss\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  early_stopping:\n",
      "    _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "    monitor: val/loss\n",
      "    min_delta: 0\n",
      "    patience: 25\n",
      "    verbose: false\n",
      "    mode: min\n",
      "    strict: true\n",
      "    check_finite: true\n",
      "    stopping_threshold: null\n",
      "    divergence_threshold: null\n",
      "    check_on_train_epoch_end: null\n",
      "  timer:\n",
      "    _target_: lightning.pytorch.callbacks.Timer\n",
      "    duration: 02:00:00:00\n",
      "    interval: epoch\n",
      "    verbose: true\n",
      "  nan_loss:\n",
      "    _target_: src.callbacks.nan_loss.NaNLossCallback\n",
      "  wandb_watcher:\n",
      "    _target_: src.callbacks.wandb.WandbTrainingCallback\n",
      "    watch: true\n",
      "    watch_log: all\n",
      "    log_freq: 100\n",
      "    log_graph: false\n",
      "  lr_monitor:\n",
      "    _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "    logging_interval: null\n",
      "    log_momentum: false\n",
      "logger:\n",
      "  csv:\n",
      "    _target_: lightning.pytorch.loggers.csv_logs.CSVLogger\n",
      "    save_dir: ${paths.output_dir}\n",
      "    name: csv/\n",
      "    prefix: ''\n",
      "  tensorboard:\n",
      "    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "    save_dir: ${paths.output_dir}/tensorboard/\n",
      "    name: null\n",
      "    log_graph: false\n",
      "    default_hp_metric: true\n",
      "    prefix: ''\n",
      "  wandb:\n",
      "    _target_: lightning.pytorch.loggers.wandb.WandbLogger\n",
      "    save_dir: ${paths.output_dir}\n",
      "    offline: false\n",
      "    id: null\n",
      "    anonymous: null\n",
      "    project: coati_small\n",
      "    log_model: true\n",
      "    prefix: ''\n",
      "    group: null\n",
      "    tags: ${tags}\n",
      "    job_type: ''\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.trainer.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 5\n",
      "  max_epochs: 100\n",
      "  accelerator: gpu\n",
      "  detect_anomaly: true\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "  log_every_n_steps: 1\n",
      "  num_sanity_val_steps: 1\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_ROOT}\n",
      "  projects_dir: ..\n",
      "  data_root_dir: ${paths.projects_dir}/cpjump1\n",
      "  metadata_path: ${paths.data_root_dir}/jump/models/metadata\n",
      "  raw_metadata_path: ${paths.data_root_dir}/jump/metadata\n",
      "  load_data_path: ${paths.data_root_dir}/jump/load_data\n",
      "  split_path: ${paths.data_root_dir}/jump/models/splits\n",
      "  log_dir: ${paths.data_root_dir}/jump/logs\n",
      "  output_dir: ./tmp/21312FS12A\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "extras:\n",
      "  ignore_warnings: true\n",
      "  style: dim\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "eval:\n",
      "  phase_I:\n",
      "    model:\n",
      "      optimizer:\n",
      "        _target_: torch.optim.Adam\n",
      "        _partial_: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.05\n",
      "        amsgrad: false\n",
      "      scheduler:\n",
      "        _target_: src.modules.lr_schedulers.warmup_wrapper.WarmUpWrapper\n",
      "        _partial_: true\n",
      "        warmup_steps:\n",
      "        - 10\n",
      "        interpolation: linear\n",
      "        wrapped_scheduler: ReduceLROnPlateau\n",
      "        cooldown: 3\n",
      "        factor: 0.6\n",
      "        patience: 7\n",
      "        min_lr: 1.0e-06\n",
      "        threshold: 0.0001\n",
      "        mode: min\n",
      "        verbose: true\n",
      "      _target_: src.eval.clinical_prediction.module.HintClinicalModulePhaseI\n",
      "      lr: 0.001\n",
      "      compound_transform: ${data.compound_transform}\n",
      "      example_input_path: ${model.example_input_path}\n",
      "    trainer:\n",
      "      _target_: lightning.pytorch.trainer.Trainer\n",
      "      default_root_dir: ${paths.output_dir}/eval/hint/phase_I/\n",
      "      min_epochs: 0\n",
      "      max_epochs: 200\n",
      "      accelerator: gpu\n",
      "      detect_anomaly: true\n",
      "      devices: ${trainer.devices}\n",
      "      check_val_every_n_epoch: 1\n",
      "      deterministic: false\n",
      "      log_every_n_steps: 1\n",
      "      num_sanity_val_steps: 1\n",
      "    callbacks:\n",
      "      rich_progress_bar:\n",
      "        _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "      model_checkpoint:\n",
      "        _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "        dirpath: ${eval.phase_I.trainer.default_root_dir}/checkpoints\n",
      "        filename: null\n",
      "        monitor: hint/phase_I/val/loss\n",
      "        verbose: false\n",
      "        save_last: false\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        auto_insert_metric_name: null\n",
      "        save_weights_only: false\n",
      "        every_n_train_steps: null\n",
      "        train_time_interval: null\n",
      "        every_n_epochs: null\n",
      "        save_on_train_epoch_end: null\n",
      "      early_stopping:\n",
      "        _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "        monitor: hint/phase_I/val/loss\n",
      "        min_delta: 0\n",
      "        patience: 15\n",
      "        verbose: false\n",
      "        mode: min\n",
      "        strict: true\n",
      "        check_finite: true\n",
      "        stopping_threshold: null\n",
      "        divergence_threshold: null\n",
      "        check_on_train_epoch_end: null\n",
      "      wandb_plotter:\n",
      "        _target_: src.callbacks.wandb.WandbPlottingCallback\n",
      "        watch: false\n",
      "        watch_log: all\n",
      "        log_freq: 50\n",
      "        plot_every_n_epoch: 4\n",
      "        log_graph: false\n",
      "        prefix: jump_moa/image/plots\n",
      "        cmap: Oranges_r\n",
      "        fmt: .2f\n",
      "        per_fmt: .1%\n",
      "        fz: 10\n",
      "        lw: 0.5\n",
      "        cbar: false\n",
      "        figsize:\n",
      "        - 16\n",
      "        - 16\n",
      "      nan_loss:\n",
      "        _target_: src.callbacks.nan_loss.NaNLossCallback\n",
      "      lr_monitor:\n",
      "        _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "        logging_interval: null\n",
      "        log_momentum: false\n",
      "    datamodule:\n",
      "      _target_: src.eval.clinical_prediction.datamodule.HintClinicalDataModulePhaseI\n",
      "      hint_dir: ${paths.data_root_dir}/hint/\n",
      "      smiless_col: smiless\n",
      "      label_col: label\n",
      "      batch_size: 256\n",
      "      num_workers: 16\n",
      "      pin_memory: false\n",
      "      prefetch_factor: 3\n",
      "      drop_last: false\n",
      "    evaluator:\n",
      "      _target_: src.eval.evaluators.Evaluator\n",
      "      visualize_kwargs: null\n",
      "  phase_II:\n",
      "    model:\n",
      "      optimizer:\n",
      "        _target_: torch.optim.Adam\n",
      "        _partial_: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.05\n",
      "        amsgrad: false\n",
      "      scheduler:\n",
      "        _target_: src.modules.lr_schedulers.warmup_wrapper.WarmUpWrapper\n",
      "        _partial_: true\n",
      "        warmup_steps:\n",
      "        - 10\n",
      "        interpolation: linear\n",
      "        wrapped_scheduler: ReduceLROnPlateau\n",
      "        cooldown: 3\n",
      "        factor: 0.6\n",
      "        patience: 7\n",
      "        min_lr: 1.0e-06\n",
      "        threshold: 0.0001\n",
      "        mode: min\n",
      "        verbose: true\n",
      "      _target_: src.eval.clinical_prediction.module.HintClinicalModulePhaseII\n",
      "      lr: 0.001\n",
      "      compound_transform: ${data.compound_transform}\n",
      "      example_input_path: ${model.example_input_path}\n",
      "    trainer:\n",
      "      _target_: lightning.pytorch.trainer.Trainer\n",
      "      default_root_dir: ${paths.output_dir}/eval/hint/phase_II/\n",
      "      min_epochs: 0\n",
      "      max_epochs: 200\n",
      "      accelerator: gpu\n",
      "      detect_anomaly: true\n",
      "      devices: ${trainer.devices}\n",
      "      check_val_every_n_epoch: 1\n",
      "      deterministic: false\n",
      "      log_every_n_steps: 1\n",
      "      num_sanity_val_steps: 1\n",
      "    callbacks:\n",
      "      rich_progress_bar:\n",
      "        _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "      model_checkpoint:\n",
      "        _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "        dirpath: ${eval.phase_II.trainer.default_root_dir}/checkpoints\n",
      "        filename: null\n",
      "        monitor: hint/phase_I/val/loss\n",
      "        verbose: false\n",
      "        save_last: false\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        auto_insert_metric_name: null\n",
      "        save_weights_only: false\n",
      "        every_n_train_steps: null\n",
      "        train_time_interval: null\n",
      "        every_n_epochs: null\n",
      "        save_on_train_epoch_end: null\n",
      "      early_stopping:\n",
      "        _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "        monitor: hint/phase_I/val/loss\n",
      "        min_delta: 0\n",
      "        patience: 15\n",
      "        verbose: false\n",
      "        mode: min\n",
      "        strict: true\n",
      "        check_finite: true\n",
      "        stopping_threshold: null\n",
      "        divergence_threshold: null\n",
      "        check_on_train_epoch_end: null\n",
      "      wandb_plotter:\n",
      "        _target_: src.callbacks.wandb.WandbPlottingCallback\n",
      "        watch: false\n",
      "        watch_log: all\n",
      "        log_freq: 50\n",
      "        plot_every_n_epoch: 4\n",
      "        log_graph: false\n",
      "        prefix: jump_moa/image/plots\n",
      "        cmap: Oranges_r\n",
      "        fmt: .2f\n",
      "        per_fmt: .1%\n",
      "        fz: 10\n",
      "        lw: 0.5\n",
      "        cbar: false\n",
      "        figsize:\n",
      "        - 16\n",
      "        - 16\n",
      "      nan_loss:\n",
      "        _target_: src.callbacks.nan_loss.NaNLossCallback\n",
      "      lr_monitor:\n",
      "        _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "        logging_interval: null\n",
      "        log_momentum: false\n",
      "    datamodule:\n",
      "      _target_: src.eval.clinical_prediction.datamodule.HintClinicalDataModulePhaseII\n",
      "      hint_dir: ${paths.data_root_dir}/hint/\n",
      "      smiless_col: smiless\n",
      "      label_col: label\n",
      "      batch_size: 256\n",
      "      num_workers: 16\n",
      "      pin_memory: false\n",
      "      prefetch_factor: 3\n",
      "      drop_last: false\n",
      "    evaluator:\n",
      "      _target_: src.eval.evaluators.Evaluator\n",
      "      visualize_kwargs: null\n",
      "  phase_III:\n",
      "    model:\n",
      "      optimizer:\n",
      "        _target_: torch.optim.Adam\n",
      "        _partial_: true\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.05\n",
      "        amsgrad: false\n",
      "      scheduler:\n",
      "        _target_: src.modules.lr_schedulers.warmup_wrapper.WarmUpWrapper\n",
      "        _partial_: true\n",
      "        warmup_steps:\n",
      "        - 10\n",
      "        interpolation: linear\n",
      "        wrapped_scheduler: ReduceLROnPlateau\n",
      "        cooldown: 3\n",
      "        factor: 0.6\n",
      "        patience: 7\n",
      "        min_lr: 1.0e-06\n",
      "        threshold: 0.0001\n",
      "        mode: min\n",
      "        verbose: true\n",
      "      _target_: src.eval.clinical_prediction.module.HintClinicalModulePhaseIII\n",
      "      lr: 0.001\n",
      "      compound_transform: ${data.compound_transform}\n",
      "      example_input_path: ${model.example_input_path}\n",
      "    trainer:\n",
      "      _target_: lightning.pytorch.trainer.Trainer\n",
      "      default_root_dir: ${paths.output_dir}/eval/hint/phase_III/\n",
      "      min_epochs: 0\n",
      "      max_epochs: 200\n",
      "      accelerator: gpu\n",
      "      detect_anomaly: true\n",
      "      devices: ${trainer.devices}\n",
      "      check_val_every_n_epoch: 1\n",
      "      deterministic: false\n",
      "      log_every_n_steps: 1\n",
      "      num_sanity_val_steps: 1\n",
      "    callbacks:\n",
      "      rich_progress_bar:\n",
      "        _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "      model_checkpoint:\n",
      "        _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "        dirpath: ${eval.phase_III.trainer.default_root_dir}/checkpoints\n",
      "        filename: null\n",
      "        monitor: hint/phase_III/val/loss\n",
      "        verbose: false\n",
      "        save_last: false\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        auto_insert_metric_name: null\n",
      "        save_weights_only: false\n",
      "        every_n_train_steps: null\n",
      "        train_time_interval: null\n",
      "        every_n_epochs: null\n",
      "        save_on_train_epoch_end: null\n",
      "      early_stopping:\n",
      "        _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "        monitor: hint/phase_III/val/loss\n",
      "        min_delta: 0\n",
      "        patience: 15\n",
      "        verbose: false\n",
      "        mode: min\n",
      "        strict: true\n",
      "        check_finite: true\n",
      "        stopping_threshold: null\n",
      "        divergence_threshold: null\n",
      "        check_on_train_epoch_end: null\n",
      "      wandb_plotter:\n",
      "        _target_: src.callbacks.wandb.WandbPlottingCallback\n",
      "        watch: false\n",
      "        watch_log: all\n",
      "        log_freq: 50\n",
      "        plot_every_n_epoch: 4\n",
      "        log_graph: false\n",
      "        prefix: jump_moa/image/plots\n",
      "        cmap: Oranges_r\n",
      "        fmt: .2f\n",
      "        per_fmt: .1%\n",
      "        fz: 10\n",
      "        lw: 0.5\n",
      "        cbar: false\n",
      "        figsize:\n",
      "        - 16\n",
      "        - 16\n",
      "      nan_loss:\n",
      "        _target_: src.callbacks.nan_loss.NaNLossCallback\n",
      "      lr_monitor:\n",
      "        _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "        logging_interval: null\n",
      "        log_momentum: false\n",
      "    datamodule:\n",
      "      _target_: src.eval.clinical_prediction.datamodule.HintClinicalDataModulePhaseIII\n",
      "      hint_dir: ${paths.data_root_dir}/hint/\n",
      "      smiless_col: smiless\n",
      "      label_col: label\n",
      "      batch_size: 256\n",
      "      num_workers: 16\n",
      "      pin_memory: false\n",
      "      prefetch_factor: 3\n",
      "      drop_last: false\n",
      "    evaluator:\n",
      "      _target_: src.eval.evaluators.Evaluator\n",
      "      visualize_kwargs: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = compose(\n",
    "    config_name=\"train.yaml\",\n",
    "    overrides=[\n",
    "        \"evaluate=true\",\n",
    "        \"eval=hint\",\n",
    "        \"paths.projects_dir=..\",\n",
    "        \"paths.output_dir=./tmp/21312FS12A\",\n",
    "        \"experiment=coati/small\",\n",
    "        \"data.batch_size=32\",\n",
    "        # \"model/molecule_encoder=gin_masking.yaml\",\n",
    "        \"trainer.devices=1\",\n",
    "        # \"eval.moa_image_task.datamodule.data_root_dir=../\",\n",
    "    ],\n",
    ")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "dm = instantiate(cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "dl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from s3://terray-public/models/grande_closed.pkl\n",
      "Loading tokenizer may_closedparen from s3://terray-public/models/grande_closed.pkl\n",
      "number of parameters: 12.64M\n",
      "number of parameters Total: 2.44M xformer: 17.92M Total: 20.36M \n",
      "vocab_name not found in tokenizer_vocabs, trying to load from file\n"
     ]
    }
   ],
   "source": [
    "model = instantiate(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicJUMPModule(\n",
       "  (image_encoder): CNNEncoder(\n",
       "    (backbone): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "    (entry): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (projection_head): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (dropouts): ModuleList(\n",
       "      (0-4): 5 x Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (molecule_encoder): COATI(\n",
       "    (backbone): RotarySmilesTransformer(\n",
       "      (norm_embed): Identity()\n",
       "      (emb): RotaryEmbedding(\n",
       "        (tok_emb): Embedding(10322, 256)\n",
       "      )\n",
       "      (transformer): ModuleDict(\n",
       "        (h): ModuleList(\n",
       "          (0-15): 16 x RotaryBlock(\n",
       "            (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): RotarySelfAttention(\n",
       "              (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlpf): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): NewGELU()\n",
       "              (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=256, out_features=10322, bias=False)\n",
       "    )\n",
       "    (projection_head): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): InfoNCE()\n",
       "  (image_backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (molecule_backbone): RotarySmilesTransformer(\n",
       "    (norm_embed): Identity()\n",
       "    (emb): RotaryEmbedding(\n",
       "      (tok_emb): Embedding(10322, 256)\n",
       "    )\n",
       "    (transformer): ModuleDict(\n",
       "      (h): ModuleList(\n",
       "        (0-15): 16 x RotaryBlock(\n",
       "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): RotarySelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlpf): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): NewGELU()\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=256, out_features=10322, bias=False)\n",
       "  )\n",
       "  (image_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (molecule_head): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (train_loss): MeanMetric()\n",
       "  (val_loss): MeanMetric()\n",
       "  (test_loss): MeanMetric()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jump_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
