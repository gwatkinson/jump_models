{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X CLIP loss reimplementation\n",
    "\n",
    "Inspired by https://github.com/lucidrains/x-clip/blob/main/x_clip/x_clip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lightning.pytorch import LightningModule\n",
    "from lion_pytorch import Lion\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    Normalize,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    ToTensor,\n",
    ")\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    ViTImageProcessor,\n",
    "    ViTMAEConfig,\n",
    "    ViTMAEForPreTraining,\n",
    ")\n",
    "\n",
    "from src.mae.module import MAEDatasetConfig, MAEModule, MAEOptimizerConfig\n",
    "from src.modules.transforms import ComplexTransform, SimpleTransform\n",
    "from src.modules.transforms.color_jitter import ColorJitterPerChannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounting cpjump1...\n",
      "Mounting cpjump2...\n",
      "Mounting cpjump3...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    if not Path(f\"../cpjump{i}/jump/\").exists():\n",
    "        print(f\"Mounting cpjump{i}...\")\n",
    "        os.system(f\"sshfs bioclust:/projects/cpjump{i}/ ../cpjump{i}\")\n",
    "    else:\n",
    "        print(f\"cpjump{i} already mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_diag(t):\n",
    "    device = t.device\n",
    "    i, j = t.shape[-2:]\n",
    "    num_diag_el = min(i, j)\n",
    "    i_range = torch.arange(i, device=device)\n",
    "    j_range = torch.arange(j, device=device)\n",
    "    diag_mask = rearrange(i_range, \"i -> i 1\") == rearrange(j_range, \"j -> 1 j\")\n",
    "    diag_el = t.masked_select(diag_mask)\n",
    "    return rearrange(diag_el, \"(b d) -> b d\", d=num_diag_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.3603,  1.5134],\n",
       "          [ 0.2363,  0.9996]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3233,  0.9008],\n",
       "          [ 1.5954, -0.4311]]],\n",
       "\n",
       "\n",
       "        [[[-0.0736, -0.4979],\n",
       "          [ 0.0264, -0.5729]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(4, 3, 1, 2, 2)\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3603,  0.9996],\n",
       "        [ 0.3233, -0.4311],\n",
       "        [-0.0736, -0.5729],\n",
       "        [ 0.1492,  1.0441],\n",
       "        [ 1.1774,  1.4675],\n",
       "        [ 0.2147, -0.7641],\n",
       "        [ 1.3447,  0.3734],\n",
       "        [ 1.5712,  1.1409],\n",
       "        [-1.6532,  0.5066],\n",
       "        [ 2.3777, -0.3648],\n",
       "        [ 0.5140, -0.1949],\n",
       "        [-0.5391,  0.4937]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_diag(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jump_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
