{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import molfeat\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "\n",
    "from src import utils\n",
    "from src.models.jump_cl import BasicJUMPModule\n",
    "from src.modules.collate_fn import default_collate\n",
    "from src.modules.losses.autoencoder_losses import GraphImageVariatonalEncoderLoss, ImageGraphVariatonalEncoderLoss\n",
    "from src.modules.losses.base_losses import CombinationLoss, RegularizationLoss\n",
    "from src.modules.losses.contrastive_losses import InfoNCE, NTXent, RegInfoNCE, RegNTXent\n",
    "from src.modules.losses.matching_losses import GraphImageMatchingLoss\n",
    "from src.utils import instantiate_evaluator_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpjump1 already mounted.\n",
      "cpjump2 already mounted.\n",
      "cpjump3 already mounted.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    if not Path(f\"../cpjump{i}/jump/\").exists():\n",
    "        print(f\"Mounting cpjump{i}...\")\n",
    "        os.system(f\"sshfs bioclust:/projects/cpjump{i}/ ../cpjump{i}\")\n",
    "    else:\n",
    "        print(f\"cpjump{i} already mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config and instantiate the model, loggers and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"../cpjump1/jump/logs/train/runs/2023-09-08_13-41-04\"\n",
    "ckpt = f\"{run}/checkpoints/epoch_097.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(version_base=None, config_path=f\"../{run}/.hydra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.yaml', 'hydra.yaml', 'overrides.yaml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f\"{run}/.hydra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_name: train\n",
      "tags:\n",
      "- big_images\n",
      "- big_jump_cl\n",
      "- pretrained\n",
      "- clip_like\n",
      "- pna\n",
      "- resnet34\n",
      "train: true\n",
      "load_first_bacth: true\n",
      "test: true\n",
      "evaluate: true\n",
      "compile: false\n",
      "ckpt_path: null\n",
      "seed: 12345\n",
      "data:\n",
      "  compound_transform:\n",
      "    _target_: src.modules.compound_transforms.pna.PNATransform\n",
      "    compound_str_type: inchi\n",
      "  _target_: src.models.jump_cl.datamodule.BasicJUMPDataModule\n",
      "  batch_size: 4\n",
      "  num_workers: 8\n",
      "  pin_memory: null\n",
      "  prefetch_factor: 2\n",
      "  drop_last: true\n",
      "  transform:\n",
      "    _target_: src.modules.transforms.SimpleTransform\n",
      "    _convert_: object\n",
      "    size: 512\n",
      "  force_split: false\n",
      "  splitter:\n",
      "    _target_: src.splitters.ScaffoldSplitter\n",
      "    train: -1\n",
      "    test: 8192\n",
      "    val: 4096\n",
      "    retrieval: 4096\n",
      "  use_compond_cache: false\n",
      "  data_root_dir: ${paths.projects_dir}/\n",
      "  split_path: ${paths.split_path}/scaffold_split/\n",
      "  dataloader_config:\n",
      "    train:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: ${data.drop_last}\n",
      "      shuffle: true\n",
      "    val:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: true\n",
      "      shuffle: false\n",
      "    test:\n",
      "      batch_size: ${data.batch_size}\n",
      "      num_workers: ${data.num_workers}\n",
      "      pin_memory: ${data.pin_memory}\n",
      "      prefetch_factor: ${data.prefetch_factor}\n",
      "      drop_last: true\n",
      "      shuffle: false\n",
      "  image_metadata_path: ${paths.metadata_path}/images_metadata.parquet\n",
      "  compound_metadata_path: ${paths.metadata_path}/compound_dict.json\n",
      "  compound_col: Metadata_InChI\n",
      "  image_sampler: null\n",
      "  metadata_dir: ${paths.raw_metadata_path}/complete_metadata.csv\n",
      "  local_load_data_dir: ${paths.load_data_path}/final/\n",
      "  index_str: '{Metadata_Source}__{Metadata_Batch}__{Metadata_Plate}__{Metadata_Well}__{Metadata_Site}'\n",
      "  channels:\n",
      "  - DNA\n",
      "  - AGP\n",
      "  - ER\n",
      "  - Mito\n",
      "  - RNA\n",
      "  col_fstring: FileName_Orig{channel}\n",
      "  id_cols:\n",
      "  - Metadata_Source\n",
      "  - Metadata_Batch\n",
      "  - Metadata_Plate\n",
      "  - Metadata_Well\n",
      "  extra_cols:\n",
      "  - Metadata_PlateType\n",
      "  - Metadata_Site\n",
      "  train_ids_name: train_big\n",
      "model:\n",
      "  image_encoder:\n",
      "    _target_: src.modules.images.timm_pretrained.CNNEncoder\n",
      "    instance_model_name: resnet34\n",
      "    target_num: ${model.embedding_dim}\n",
      "    n_channels: 5\n",
      "    pretrained: true\n",
      "  molecule_encoder:\n",
      "    _target_: src.modules.molecules.pna.PNA\n",
      "    ckpt_path: ${paths.projects_dir}/cpjump1/jump/s3_cache/best_checkpoint_35epochs.pt\n",
      "    out_dim: ${model.embedding_dim}\n",
      "    target_dim: 256\n",
      "    hidden_dim: 200\n",
      "    mid_batch_norm: true\n",
      "    last_batch_norm: true\n",
      "    readout_batchnorm: true\n",
      "    batch_norm_momentum: 0.93\n",
      "    readout_hidden_dim: 200\n",
      "    readout_layers: 2\n",
      "    dropout: 0.05\n",
      "    propagation_depth: 7\n",
      "    aggregators:\n",
      "    - mean\n",
      "    - max\n",
      "    - min\n",
      "    - std\n",
      "    scalers:\n",
      "    - identity\n",
      "    - amplification\n",
      "    - attenuation\n",
      "    readout_aggregators:\n",
      "    - min\n",
      "    - max\n",
      "    - mean\n",
      "    pretrans_layers: 2\n",
      "    posttrans_layers: 1\n",
      "    residual: true\n",
      "  criterion:\n",
      "    _target_: src.modules.losses.contrastive_losses.RegNTXent\n",
      "    norm: true\n",
      "    temperature: 0.5\n",
      "    return_rank: true\n",
      "    temperature_requires_grad: false\n",
      "    alpha: 0.05\n",
      "    mse_reg: 0.1\n",
      "    l1_reg: 0.1\n",
      "    uniformity_reg: 0\n",
      "    variance_reg: 1\n",
      "    covariance_reg: 0.25\n",
      "    temperature_min: 0\n",
      "    temperature_max: 100\n",
      "  optimizer:\n",
      "    _target_: torch.optim.AdamW\n",
      "    _partial_: true\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "    eps: 1.0e-08\n",
      "    weight_decay: 0.01\n",
      "    amsgrad: false\n",
      "    lr: ${model.lr}\n",
      "  scheduler:\n",
      "    _target_: src.modules.lr_schedulers.cosine_lr.LinearWarmupCosineAnnealingLR\n",
      "    _partial_: true\n",
      "    warmup_epochs: 10\n",
      "    max_epochs: ${trainer.max_epochs}\n",
      "    warmup_start_lr: 1.0e-06\n",
      "    eta_min: 0.0\n",
      "    last_epoch: -1\n",
      "  _target_: src.models.jump_cl.module.BasicJUMPModule\n",
      "  embedding_dim: 512\n",
      "  lr: 0.0003\n",
      "  batch_size: ${data.batch_size}\n",
      "  example_input_path: null\n",
      "  monitor: val/loss\n",
      "  interval: epoch\n",
      "  frequency: 1\n",
      "  image_backbone: backbone\n",
      "  image_head: projection_head\n",
      "  molecule_backbone: backbone\n",
      "  molecule_head: projection_head\n",
      "  split_lr_in_groups: false\n",
      "callbacks:\n",
      "  rich_progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.RichProgressBar\n",
      "  model_summary:\n",
      "    _target_: lightning.pytorch.callbacks.RichModelSummary\n",
      "    max_depth: 2\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}/checkpoints\n",
      "    filename: epoch_{epoch:03d}\n",
      "    monitor: val/loss\n",
      "    verbose: false\n",
      "    save_last: true\n",
      "    save_top_k: 1\n",
      "    mode: min\n",
      "    auto_insert_metric_name: false\n",
      "    save_weights_only: false\n",
      "    every_n_train_steps: null\n",
      "    train_time_interval: null\n",
      "    every_n_epochs: null\n",
      "    save_on_train_epoch_end: null\n",
      "  early_stopping:\n",
      "    _target_: lightning.pytorch.callbacks.EarlyStopping\n",
      "    monitor: val/loss\n",
      "    min_delta: 0\n",
      "    patience: 30\n",
      "    verbose: false\n",
      "    mode: min\n",
      "    strict: true\n",
      "    check_finite: true\n",
      "    stopping_threshold: null\n",
      "    divergence_threshold: null\n",
      "    check_on_train_epoch_end: null\n",
      "  timer:\n",
      "    _target_: lightning.pytorch.callbacks.Timer\n",
      "    duration: 02:00:00:00\n",
      "    interval: epoch\n",
      "    verbose: true\n",
      "  nan_loss:\n",
      "    _target_: src.callbacks.nan_loss.NaNLossCallback\n",
      "  wandb_watcher:\n",
      "    _target_: src.callbacks.wandb.WandbTrainingCallback\n",
      "    watch: true\n",
      "    watch_log: all\n",
      "    log_freq: 100\n",
      "    log_graph: false\n",
      "  temperature_logger:\n",
      "    _target_: src.callbacks.temperature_log.TemperatureLoggingCallback\n",
      "    attribute_name:\n",
      "    - criterion\n",
      "    - temperature\n",
      "    key: model/temperature\n",
      "    interval: step\n",
      "    frequency: 1\n",
      "  lr_monitor:\n",
      "    _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "    logging_interval: null\n",
      "    log_momentum: false\n",
      "logger:\n",
      "  csv:\n",
      "    _target_: lightning.pytorch.loggers.csv_logs.CSVLogger\n",
      "    save_dir: ${paths.output_dir}\n",
      "    name: csv/\n",
      "    prefix: ''\n",
      "  tensorboard:\n",
      "    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "    save_dir: ${paths.output_dir}/tensorboard/\n",
      "    name: null\n",
      "    log_graph: false\n",
      "    default_hp_metric: true\n",
      "    prefix: ''\n",
      "  wandb:\n",
      "    _target_: lightning.pytorch.loggers.wandb.WandbLogger\n",
      "    save_dir: ${paths.output_dir}\n",
      "    offline: false\n",
      "    id: null\n",
      "    anonymous: null\n",
      "    project: big_images\n",
      "    log_model: true\n",
      "    prefix: ''\n",
      "    group: null\n",
      "    tags: ${tags}\n",
      "    job_type: ''\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.trainer.Trainer\n",
      "  default_root_dir: ${paths.output_dir}\n",
      "  min_epochs: 5\n",
      "  max_epochs: 100\n",
      "  accelerator: gpu\n",
      "  detect_anomaly: true\n",
      "  devices: 1\n",
      "  check_val_every_n_epoch: 1\n",
      "  deterministic: false\n",
      "  strategy: ddp\n",
      "  num_nodes: 1\n",
      "  sync_batchnorm: false\n",
      "  log_every_n_steps: 1\n",
      "  num_sanity_val_steps: 1\n",
      "paths:\n",
      "  root_dir: ${oc.env:PROJECT_ROOT}\n",
      "  projects_dir: ..\n",
      "  data_root_dir: ${paths.projects_dir}/cpjump1\n",
      "  metadata_path: ${paths.data_root_dir}/jump/models/metadata\n",
      "  raw_metadata_path: ${paths.data_root_dir}/jump/metadata\n",
      "  load_data_path: ${paths.data_root_dir}/jump/load_data\n",
      "  model_dir: ${paths.data_root_dir}/jump/s3_cache\n",
      "  split_path: ${paths.data_root_dir}/jump/models/splits\n",
      "  log_dir: ${paths.data_root_dir}/jump/logs\n",
      "  output_dir: ../cpjump1/jump/logs/train/runs/../cpjump1/jump/logs/train/runs/2023-09-08_13-41-04\n",
      "  work_dir: ${hydra:runtime.cwd}\n",
      "extras:\n",
      "  ignore_warnings: true\n",
      "  style: dim\n",
      "  enforce_tags: true\n",
      "  print_config: true\n",
      "eval: retrieval\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = compose(\n",
    "    config_name=\"config.yaml\",\n",
    "    overrides=[\n",
    "        \"evaluate=true\",\n",
    "        \"eval=retrieval\",\n",
    "        \"paths.projects_dir=..\",\n",
    "        f\"paths.output_dir=../cpjump1/jump/logs/train/runs/{run}\",\n",
    "        # \"experiment=fp_big\",\n",
    "        \"data.batch_size=4\",\n",
    "        # \"model/molecule_encoder=gin_masking.yaml\",\n",
    "        \"trainer.devices=1\",\n",
    "        # \"eval.moa_image_task.datamodule.data_root_dir=../\",\n",
    "    ],\n",
    ")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "dm = instantiate(cfg.data)\n",
    "dm.prepare_data()\n",
    "dm.setup(\"test\")\n",
    "dl = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "cfg.model[\"_target_\"] += \".load_from_checkpoint\"\n",
    "with open_dict(cfg.model):\n",
    "    cfg.model[\"checkpoint_path\"] = ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:161: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['criterion.losses.temp_loss.temperature']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model = instantiate(cfg.model, map_location=device, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "embs = []\n",
    "for i, batch in enumerate(dl):\n",
    "    batches.append({k: v.to(device) for k, v in batch.items()})\n",
    "    embs.append(model(**batches[i]))\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = instantiate(cfg.model.criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegNTXent(\n",
       "  (losses): ModuleDict(\n",
       "    (temp_loss): NTXent()\n",
       "    (reg_loss): RegularizationLoss(\n",
       "      (mse_loss): MSELoss()\n",
       "      (l1_loss): L1Loss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ClampedParameter>: 0.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.losses[\"temp_loss\"].temperature_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.5000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, comp1 = embs[0][\"image_emb\"], embs[0][\"compound_emb\"]\n",
    "img2, comp2 = embs[1][\"image_emb\"], embs[1][\"compound_emb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RegNTXent/loss': tensor(0.4345, device='cuda:0', grad_fn=<NegBackward0>),\n",
       " 'RegNTXent/x_to_y_top1': tensor(0.7500, device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_top5': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_top10': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_mean_pos': tensor(1.5000, device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_mean_pos_normed': tensor(0.3750, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top1': tensor(0.5000, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top5': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top10': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_mean_pos': tensor(1.5000, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_mean_pos_normed': tensor(0.3750, device='cuda:0'),\n",
       " 'Regularization/mse_loss': tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'Regularization/std_loss': tensor(1.9144, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'Regularization/cov_loss': tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'Regularization/loss': tensor(1.9152, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'loss': tensor(0.5085, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(img1, comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RegNTXent/loss': tensor(0.4503, device='cuda:0', grad_fn=<NegBackward0>),\n",
       " 'RegNTXent/x_to_y_top1': tensor(0.5000, device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_top5': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_top10': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_mean_pos': tensor(1.5000, device='cuda:0'),\n",
       " 'RegNTXent/x_to_y_mean_pos_normed': tensor(0.3750, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top1': tensor(0.5000, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top5': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_top10': tensor(1., device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_mean_pos': tensor(1.5000, device='cuda:0'),\n",
       " 'RegNTXent/y_to_x_mean_pos_normed': tensor(0.3750, device='cuda:0'),\n",
       " 'Regularization/mse_loss': tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'Regularization/std_loss': tensor(1.9184, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'Regularization/cov_loss': tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'Regularization/loss': tensor(1.9193, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'loss': tensor(0.5237, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(img2, comp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphImageVariatonalEncoder(\n",
       "  (criterion): CosineSimilarity()\n",
       "  (fc_mu): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2i_vae_loss = GraphImageVariatonalEncoderLoss(emb_dim=512, similarity=\"cosine\", beta=1.0, detach_target=False)\n",
    "g2i_vae_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageGraphVariatonalEncoder(\n",
       "  (criterion): CosineSimilarity()\n",
       "  (fc_mu): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc_var): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2g_vae_loss = ImageGraphVariatonalEncoderLoss(emb_dim=512, similarity=\"cosine\", beta=1.0, detach_target=False)\n",
    "i2g_vae_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruction_loss': tensor(0.0184, grad_fn=<MeanBackward0>),\n",
       " 'kl_loss': tensor(0.1196, grad_fn=<MeanBackward1>),\n",
       " 'loss': tensor(0.1380, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2g_vae_loss(img2, comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruction_loss': tensor(-0.0020, grad_fn=<MeanBackward0>),\n",
       " 'kl_loss': tensor(0.1290, grad_fn=<MeanBackward1>),\n",
       " 'loss': tensor(0.1269, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2i_vae_loss(img2, comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mGraphImageMatchingLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GraphImageMatchingLoss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfusion_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /mnt/2547d4d7-6732-4154-b0e1-17b0c1e0c565/Document-2/Projet2/Stage/workspace/jump_models/src/modules/losses/matching_losses.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "GraphImageMatchingLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphImageMatchingLoss(\n",
       "  (fusion_layer): DeepSetFusion(\n",
       "    (image_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (graph_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (fusion): DeepsetFusionWithTransformer(\n",
       "      (projections): ModuleDict(\n",
       "        (image): Identity()\n",
       "        (graph): Identity()\n",
       "      )\n",
       "      (attention): Identity()\n",
       "      (pooling_function): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (auroc): BinaryAUROC()\n",
       "  (accuracy): BinaryAccuracy()\n",
       "  (recall): BinaryRecall()\n",
       "  (precision): BinaryPrecision()\n",
       "  (f1_score): BinaryF1Score()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gim = GraphImageMatchingLoss(embedding_dim=512, fusion_layer=\"deepset\")\n",
    "gim.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntxent = NTXent(\n",
    "    temperature=0.1,\n",
    "    return_rank=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = CombinationLoss(\n",
    "    losses={\n",
    "        \"gim\": gim,\n",
    "        \"i2g_ve\": i2g_vae_loss,\n",
    "        \"g2i_ve\": g2i_vae_loss,\n",
    "        \"ntxent\": ntxent,\n",
    "    },\n",
    "    weights=[1.0, 1.0, 1.0, 1.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gim/loss': tensor(0.7045, grad_fn=<NllLossBackward0>),\n",
       " 'gim/auroc': tensor(0.4062),\n",
       " 'gim/accuracy': tensor(0.5833),\n",
       " 'gim/recall': tensor(0.5000),\n",
       " 'gim/precision': tensor(0.4000),\n",
       " 'gim/f1_score': tensor(0.4444),\n",
       " 'i2g_ve/reconstruction_loss': tensor(0.0099, grad_fn=<MeanBackward0>),\n",
       " 'i2g_ve/kl_loss': tensor(0.1223, grad_fn=<MeanBackward1>),\n",
       " 'i2g_ve/loss': tensor(0.1322, grad_fn=<AddBackward0>),\n",
       " 'g2i_ve/reconstruction_loss': tensor(-0.0067, grad_fn=<MeanBackward0>),\n",
       " 'g2i_ve/kl_loss': tensor(0.1290, grad_fn=<MeanBackward1>),\n",
       " 'g2i_ve/loss': tensor(0.1223, grad_fn=<AddBackward0>),\n",
       " 'ntxent/loss': tensor(3.9879, grad_fn=<NegBackward0>),\n",
       " 'ntxent/x_to_y_top1': tensor(0.),\n",
       " 'ntxent/x_to_y_top5': tensor(1.),\n",
       " 'ntxent/x_to_y_top10': tensor(1.),\n",
       " 'ntxent/x_to_y_mean_pos': tensor(3.7500),\n",
       " 'ntxent/x_to_y_mean_pos_normed': tensor(0.9375),\n",
       " 'ntxent/y_to_x_top1': tensor(0.2500),\n",
       " 'ntxent/y_to_x_top5': tensor(1.),\n",
       " 'ntxent/y_to_x_top10': tensor(1.),\n",
       " 'ntxent/y_to_x_mean_pos': tensor(3.),\n",
       " 'ntxent/y_to_x_mean_pos_normed': tensor(0.7500),\n",
       " 'loss': tensor(4.9469, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination(img1, comp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gim/loss': tensor(0.7706, grad_fn=<NllLossBackward0>),\n",
       " 'gim/auroc': tensor(0.4375),\n",
       " 'gim/accuracy': tensor(0.6667),\n",
       " 'gim/recall': tensor(0.),\n",
       " 'gim/precision': tensor(0.),\n",
       " 'gim/f1_score': tensor(0.),\n",
       " 'i2g_ve/reconstruction_loss': tensor(0.0179, grad_fn=<MeanBackward0>),\n",
       " 'i2g_ve/kl_loss': tensor(0.1196, grad_fn=<MeanBackward1>),\n",
       " 'i2g_ve/loss': tensor(0.1376, grad_fn=<AddBackward0>),\n",
       " 'g2i_ve/reconstruction_loss': tensor(0.0304, grad_fn=<MeanBackward0>),\n",
       " 'g2i_ve/kl_loss': tensor(0.1300, grad_fn=<MeanBackward1>),\n",
       " 'g2i_ve/loss': tensor(0.1604, grad_fn=<AddBackward0>),\n",
       " 'ntxent/loss': tensor(6.6429, grad_fn=<NegBackward0>),\n",
       " 'ntxent/x_to_y_top1': tensor(0.),\n",
       " 'ntxent/x_to_y_top5': tensor(1.),\n",
       " 'ntxent/x_to_y_top10': tensor(1.),\n",
       " 'ntxent/x_to_y_mean_pos': tensor(3.5000),\n",
       " 'ntxent/x_to_y_mean_pos_normed': tensor(0.8750),\n",
       " 'ntxent/y_to_x_top1': tensor(0.),\n",
       " 'ntxent/y_to_x_top5': tensor(1.),\n",
       " 'ntxent/y_to_x_top10': tensor(1.),\n",
       " 'ntxent/y_to_x_mean_pos': tensor(3.2500),\n",
       " 'ntxent/y_to_x_mean_pos_normed': tensor(0.8125),\n",
       " 'loss': tensor(7.7115, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination(img2, comp2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jump_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
