{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT Top Benchmark evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gwatk/miniconda3/envs/jump_models/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from lightning import LightningDataModule\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "from src.eval.moa.datamodule import JumpMOADataModule\n",
    "from src.eval.moa.module import JumpMOAImageGraphModule, JumpMOAImageModule\n",
    "from src.modules.collate_fn import image_graph_label_collate_function, label_graph_collate_function\n",
    "from src.modules.compound_transforms import DGLPretrainedFromSmiles\n",
    "from src.modules.images.timm_pretrained import CNNEncoder\n",
    "from src.modules.molecules.dgllife_gin import GINPretrainedWithLinearHead\n",
    "from src.modules.transforms import DefaultJUMPTransform\n",
    "from src.splitters import StratifiedSplitter\n",
    "from src.utils.io import download_and_extract_zip, load_image_paths_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpjump1 already mounted.\n",
      "cpjump2 already mounted.\n",
      "cpjump3 already mounted.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    if not Path(f\"../cpjump{i}/jump/\").exists():\n",
    "        print(f\"Mounting cpjump{i}...\")\n",
    "        os.system(f\"sshfs bioclust:/projects/cpjump{i}/ ../cpjump{i}\")\n",
    "    else:\n",
    "        print(f\"cpjump{i} already mounted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"../cpjump1/jump/metadata\"\n",
    "load_data_path = \"../cpjump1/jump/load_data\"\n",
    "hint_path = \"../cpjump1/hint-clinical-trial-outcome-prediction/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compound.csv.gz',\n",
       " 'crispr.csv.gz',\n",
       " 'microscope_config.csv',\n",
       " 'microscope_filter.csv',\n",
       " 'orf.csv.gz',\n",
       " 'plate.csv.gz',\n",
       " 'README.md',\n",
       " 'well.csv.gz',\n",
       " 'compound.csv',\n",
       " 'crispr.csv',\n",
       " 'orf.csv',\n",
       " 'plate.csv',\n",
       " 'well.csv',\n",
       " 'complete_metadata.csv',\n",
       " 'resolution.csv',\n",
       " 'JUMP-Target-1_compound_metadata.tsv',\n",
       " 'JUMP-Target-1_compound_platemap.tsv',\n",
       " 'JUMP-Target-1_crispr_metadata.tsv',\n",
       " 'JUMP-Target-1_crispr_platemap.tsv',\n",
       " 'JUMP-Target-1_orf_metadata.tsv',\n",
       " 'JUMP-Target-1_orf_platemap.tsv',\n",
       " 'JUMP-Target-2_compound_metadata.tsv',\n",
       " 'JUMP-Target-2_compound_platemap.tsv',\n",
       " 'JUMP-MOA_compound_metadata.tsv',\n",
       " 'local_metadata.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADMET',\n",
       " 'NCT00000378.xml',\n",
       " 'README.md',\n",
       " 'drugbank_mini.csv',\n",
       " 'phase_III_test.csv',\n",
       " 'phase_III_train.csv',\n",
       " 'phase_III_valid.csv',\n",
       " 'phase_II_test.csv',\n",
       " 'phase_II_train.csv',\n",
       " 'phase_II_valid.csv',\n",
       " 'phase_I_test.csv',\n",
       " 'phase_I_train.csv',\n",
       " 'phase_I_valid.csv',\n",
       " 'raw_data.csv',\n",
       " 'sentence2embedding.pkl',\n",
       " 'sponsor2approvalrate.csv',\n",
       " 'sponsor2count.csv',\n",
       " 'toy_test.csv',\n",
       " 'toy_train.csv',\n",
       " 'toy_valid.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(hint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load phase csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trial_Dataset(Dataset):\n",
    "    def __init__(self, nctid_lst, label_lst, smiles_lst, icdcode_lst):  # , criteria_lst):\n",
    "        self.nctid_lst = nctid_lst\n",
    "        self.label_lst = label_lst\n",
    "        self.smiles_lst = smiles_lst\n",
    "        self.icdcode_lst = icdcode_lst\n",
    "        # self.criteria_lst = criteria_lst\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nctid_lst)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.nctid_lst[index],\n",
    "            self.label_lst[index],\n",
    "            self.smiles_lst[index],\n",
    "            self.icdcode_lst[index],\n",
    "        )  # , self.criteria_lst[index]\n",
    "\n",
    "    #### smiles_lst[index] is list of smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_three_feature_2_dataloader(csvfile, shuffle, batch_size):\n",
    "    with open(csvfile) as csvfile:\n",
    "        rows = list(csv.reader(csvfile, delimiter=\",\"))[1:]\n",
    "    ## nctid,status,why_stop,label,phase,diseases,icdcodes,drugs,smiless,criteria\n",
    "    nctid_lst = [row[0] for row in rows]\n",
    "    label_lst = [row[3] for row in rows]\n",
    "    icdcode_lst = [row[6] for row in rows]\n",
    "    drugs_lst = [row[7] for row in rows]\n",
    "    smiles_lst = [row[8] for row in rows]\n",
    "    # criteria_lst = [row[9] for row in rows]\n",
    "    dataset = Trial_Dataset(nctid_lst, label_lst, smiles_lst, icdcode_lst)  # , criteria_lst)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=trial_collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_txt_to_2lst(smiles_txt_file):\n",
    "    with open(smiles_txt_file) as fin:\n",
    "        lines = fin.readlines()\n",
    "    smiles_lst = [line.split()[0] for line in lines]\n",
    "    label_lst = [int(line.split()[1]) for line in lines]\n",
    "    return smiles_lst, label_lst\n",
    "\n",
    "\n",
    "def smiles_txt_to_lst(text):\n",
    "    \"\"\"\n",
    "    \"['CN[C@H]1CC[C@@H](C2=CC(Cl)=C(Cl)C=C2)C2=CC=CC=C12', 'CNCCC=C1C2=CC=CC=C2CCC2=CC=CC=C12']\"\n",
    "    \"\"\"\n",
    "    text = text[1:-1]\n",
    "    lst = [i.strip()[1:-1] for i in text.split(\",\")]\n",
    "    return lst\n",
    "\n",
    "\n",
    "def icdcode_text_2_lst_of_lst(text):\n",
    "    text = text[2:-2]\n",
    "    lst_lst = []\n",
    "    for i in text.split('\", \"'):\n",
    "        i = i[1:-1]\n",
    "        lst_lst.append([j.strip()[1:-1] for j in i.split(\",\")])\n",
    "    return lst_lst\n",
    "\n",
    "\n",
    "def trial_collate_fn(x):\n",
    "    nctid_lst = [i[0] for i in x]  ### ['NCT00604461', ..., 'NCT00788957']\n",
    "    label_vec = default_collate([int(i[1]) for i in x])  ### shape n,\n",
    "    smiles_lst = [smiles_txt_to_lst(i[2]) for i in x]\n",
    "    icdcode_lst = [icdcode_text_2_lst_of_lst(i[3]) for i in x]\n",
    "    # criteria_lst = [protocol2feature(i[4], sentence2vec) for i in x]\n",
    "    return [nctid_lst, label_vec, smiles_lst, icdcode_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_1_train = osp.join(hint_path, \"phase_I_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(phase_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1', '[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1']\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"smiless\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = csv_three_feature_2_dataloader(phase_1_train, False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NCT01187615', 'NCT01046487', 'NCT01381887', 'NCT02015676'],\n",
       " tensor([0, 1, 1, 1]),\n",
       " [['[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1',\n",
       "   '[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1'],\n",
       "  ['CC1=NC(NC2=NC=C(S2)C(=O)NC2=C(C)C=CC=C2Cl)=CC(=N1)N1CCN(CCO)CC1',\n",
       "   'CC1=NC(NC2=NC=C(S2)C(=O)NC2=C(C)C=CC=C2Cl)=CC(=N1)N1CCN(CCO)CC1',\n",
       "   'CC1=NC(NC2=NC=C(S2)C(=O)NC2=C(C)C=CC=C2Cl)=CC(=N1)N1CCN(CCO)CC1'],\n",
       "  ['CN1C(=O)C=C(N2CCC[C@@H](N)C2)N(CC2=C(C=CC=C2)C#N)C1=O',\n",
       "   '[H][C@]1(O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O)C1=CC=C(C)C(CC2=CC=C(S2)C2=CC=C(F)C=C2)=C1',\n",
       "   '[H][C@]1(O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O)C1=CC=C(C)C(CC2=CC=C(S2)C2=CC=C(F)C=C2)=C1',\n",
       "   '[H][C@]1(O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O)C1=CC=C(C)C(CC2=CC=C(S2)C2=CC=C(F)C=C2)=C1'],\n",
       "  ['[H][N]1([H])[C@@H]2CCCC[C@H]2[N]([H])([H])[Pt]11OC(=O)C(=O)O1',\n",
       "   '[H][C@@]1(C[C@@H](C)[C@]2([H])CC(=O)[C@H](C)\\\\\\\\C=C(C)\\\\\\\\[C@@H](O)[C@@H](OC)C(=O)[C@H](C)C[C@H](C)\\\\\\\\C=C\\\\\\\\C=C\\\\\\\\C=C(C)\\\\\\\\[C@H](C[C@]3([H])CC[C@@H](C)[C@@](O)(O3)C(=O)C(=O)N3CCCC[C@@]3([H])C(=O)O2)OC)CC[C@@H](OCCO)[C@@H](C1)OC',\n",
       "   'COC1=CC=CC2=C1C(=O)C1=C(O)C3=C(C[C@](O)(C[C@@H]3O[C@H]3C[C@H](N)[C@H](O)[C@H](C)O3)C(=O)CO)C(O)=C1C2=O']],\n",
       " [[['D02.20', 'D02.21', 'D02.22']],\n",
       "  [['C05.2', 'C10.0', 'C16.0', 'C16.4', 'C17.0', 'C17.1', 'C17.2']],\n",
       "  [['E11.65', 'E11.9', 'E11.21', 'E11.36', 'E11.41', 'E11.42', 'E11.44']],\n",
       "  [['C79.81', 'D24.1', 'D24.2', 'D24.9', 'D49.3', 'C44.501', 'D48.60']]]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jump_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
